{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MDP and gym\n",
        "\n",
        "## Evaluation considerations\n",
        "- We take into account the correctness of the solutions but also their generality and quality of the code\n",
        "- Comment and discuss on the results of all your exercises (in a cell immediately after the results). You may also state the difficulties encountered, lessons learned and your understanding of the problem and solution\n",
        "- Clean-up your code before submission, do not leave unnecessary code attempts, or if you deem it important, leave them in a way that it is easily understood and with comments/discussion\n",
        "- We also value the originality of the solutions, don't hesitate in performing unrequested additional tasks in relation to the exercises\n",
        "\n",
        "\n",
        "**NOTE**\n",
        "\n",
        "Do not try to reproduce exactly the results in this notebook. RL training is very noisy and performances of learned policies can vary a lot, try running your trains several times.\n",
        "\n"
      ],
      "metadata": {
        "id": "m7DYz11gj_cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[accept-rom-license,toy_text]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWGMi3Saqozk",
        "outputId": "547acd65-4306-4084-ffc4-8b1f13d11d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.27.1-py3-none-any.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax-jumpy>=0.2.0\n",
            "  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (4.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.12.0)\n",
            "Installing collected packages: gymnasium-notices, jax-jumpy, gymnasium\n",
            "Successfully installed gymnasium-0.27.1 gymnasium-notices-0.0.1 jax-jumpy-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,toy_text] in /usr/local/lib/python3.8/dist-packages (0.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license,toy_text]) (4.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license,toy_text]) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license,toy_text]) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license,toy_text]) (1.21.6)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license,toy_text]) (0.0.1)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license,toy_text]) (0.2.0)\n",
            "Collecting pygame==2.1.3.dev8\n",
            "  Downloading pygame-2.1.3.dev8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (4.64.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (5.10.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.4.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[accept-rom-license,toy_text]) (3.12.0)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,toy_text]) (2.10)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook"
      ],
      "metadata": {
        "id": "qWnSBnqjqZuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wrapper for recording an environment into a video\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from copy import deepcopy\n",
        "from typing import Any, SupportsFloat\n",
        "\n",
        "from gymnasium.core import ActType, ObsType, RenderFrame, WrapperActType, WrapperObsType\n",
        "from gymnasium.error import DependencyNotInstalled\n",
        "\n",
        "class RecordVideo(gym.Wrapper):\n",
        "    \"\"\"Adapted from https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/experimental/wrappers/rendering.py#L87\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Initialize a :class:`HumanRendering` instance.\n",
        "        Args:\n",
        "            env: The environment that is being wrapped\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        assert env.render_mode in [\n",
        "            \"rgb_array\",\n",
        "            \"rgb_array_list\",\n",
        "        ], f\"Expected env.render_mode to be one of 'rgb_array' or 'rgb_array_list' but got '{env.render_mode}'\"\n",
        "\n",
        "        if \"render_fps\" not in env.metadata:\n",
        "            env.metadata[\"render_fps\"] = 24\n",
        "\n",
        "        assert (\n",
        "            \"render_fps\" in env.metadata\n",
        "        ), \"The base environment must specify 'render_fps' to be used with the HumanRendering wrapper\"\n",
        "\n",
        "        if \"human\" not in self.metadata[\"render_modes\"]:\n",
        "            self.metadata = deepcopy(self.env.metadata)\n",
        "            self.metadata[\"render_modes\"].append(\"human\")\n",
        "\n",
        "        self.artists = []\n",
        "        self.figure = None\n",
        "        self.env = env\n",
        "    @property\n",
        "    def render_mode(self):\n",
        "        \"\"\"Always returns ``'human'``.\"\"\"\n",
        "        return \"human\"\n",
        "\n",
        "    def step(\n",
        "        self, action: WrapperActType\n",
        "    ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict]:\n",
        "        \"\"\"Perform a step in the base environment and render a frame to the screen.\"\"\"\n",
        "        result = super().step(action)\n",
        "        self._render_frame()\n",
        "        return result\n",
        "\n",
        "    def reset(\n",
        "        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n",
        "    ) -> tuple[WrapperObsType, dict[str, Any]]:\n",
        "        \"\"\"Reset the base environment and render a frame to the screen.\"\"\"\n",
        "        result = super().reset(seed=seed, options=options)\n",
        "        self._render_frame()\n",
        "        return result\n",
        "\n",
        "    def video(self):\n",
        "        \"\"\"This method renders all frames collected up to now.\"\"\"\n",
        "        if self.figure is not None:\n",
        "            from IPython.display import HTML\n",
        "            import matplotlib.animation\n",
        "\n",
        "            animation = matplotlib.animation.ArtistAnimation(self.figure, self.artists, \n",
        "                                                             interval=1000//self.metadata[\"render_fps\"],\n",
        "                                                             blit=True,\n",
        "                                                             repeat=True,\n",
        "                                                             repeat_delay=2000)\n",
        "            return HTML(animation.to_html5_video())\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _render_frame(self):\n",
        "        \"\"\"Fetch the last frame from the base environment and render it to the screen.\"\"\"\n",
        "        try:\n",
        "            import matplotlib.animation\n",
        "            import numpy as np\n",
        "        except ImportError:\n",
        "            raise DependencyNotInstalled(\n",
        "                \"matplotlib is not installed, run `pip install matplotlib`\"\n",
        "            )\n",
        "        if self.env.render_mode == \"rgb_array_list\":\n",
        "            rgb_arrays = self.env.render()\n",
        "        elif self.env.render_mode == \"rgb_array\":\n",
        "            rgb_arrays = [self.env.render()]\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Wrapped environment must have mode 'rgb_array' or 'rgb_array_list', actual render mode: {self.env.render_mode}\"\n",
        "            )\n",
        "\n",
        "        assert isinstance(rgb_arrays, list)\n",
        "\n",
        "        for rgb_array in rgb_arrays:\n",
        "            assert isinstance(rgb_array, np.ndarray)\n",
        "\n",
        "        if self.figure is None:\n",
        "            self.figure = plt.figure()\n",
        "            plt.axis('off')\n",
        "        \n",
        "        self.artists.append([plt.imshow(rgb_array) for rgb_array in rgb_arrays])\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the rendering window.\"\"\"\n",
        "        result = self.video()\n",
        "        super().close()\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "0Sp3p6hxP-0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNMbXXnsN6SU"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(linewidth=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's render soe steps to show how to use the `RecordVideo` class."
      ],
      "metadata": {
        "id": "dbhFTLQmSyMu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPVrTQ16f0nI"
      },
      "source": [
        "env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\"))\n",
        "env.reset()                    \n",
        "\n",
        "for _ in range(100):\n",
        "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done:\n",
        "        env.reset()\n",
        "\n",
        "display(env.video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is only needed when we want to render. In many situations of this lab we will perform many steps without the need to render, we will avoid recording a video, since doing so requires a lot of RAM (to save all the RGB frames) and we may run out of memory.\n",
        "\n",
        "To avoid this do not wrap the environment on a `RecordVideo` class and set keyword argument `render_mode=None` when making the environment."
      ],
      "metadata": {
        "id": "jmJjXplkS6dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "env.reset()                    \n",
        "\n",
        "for _ in range(100):\n",
        "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done:\n",
        "        env.reset()"
      ],
      "metadata": {
        "id": "8hymbBwZTRnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab we will be estimating value functions. Value functions of an MDP are conditioned on the policy used.\n",
        "\n",
        "A policy is the probability distribution over actions. We will restrict ourselves to discrete action spaces.\n",
        "\n",
        "We will start by using a uniform policy that chooses with equal chances between all actions.\n",
        "\n",
        "## Exercise : A uniform policy\n",
        "\n",
        "Create a `policy_uniform` function that returns the probability of each possible action given an environment and state.\n",
        "\n",
        "We will also create a `sample_multinomial` function that facilitates sampling an action from the policy given the probability of each possible action.\n",
        "\n",
        "**Hint** you may access the number of discrete actions with `env.action_space.n`."
      ],
      "metadata": {
        "id": "IY9cAVvmE3SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UniformPolicy(object):\n",
        "    def __init__(self, action_space):\n",
        "        assert isinstance(action_space, gym.spaces.discrete.Discrete), \"Can only create uniform policies for Discrete action spaces\"\n",
        "        # print(action_space)\n",
        "        self.n_actions = action_space.n\n",
        "        self.training = True\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def probability(self, state, action):\n",
        "        ### BEGIN SOLUTION\n",
        "        action_prob = 1/self.n_actions\n",
        "        ### END SOLUTION\n",
        "\n",
        "        return action_prob\n",
        "\n",
        "    def sample(self, state):\n",
        "        ### BEGIN SOLUTION\n",
        "        choices = [i for i in range(self.n_actions)]\n",
        "        # print(state)\n",
        "        action = np.random.choice(choices)\n",
        "        # action = np.random.randint(self.n_actions)\n",
        "        ### END SOLUTION\n",
        "        \n",
        "        return action\n",
        "\n",
        "# Let's instantiate a uniform policy\n",
        "env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\"))\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "\n",
        "# And sample 20 actions from state==0\n",
        "actions = [uniform_policy.sample(0) for i in range(20)]\n",
        "print(actions)"
      ],
      "metadata": {
        "id": "Za0fSou4E7X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTAXmkNOUJ2a"
      },
      "source": [
        "Gym follows a formalism of MDPs that differs slightly (is more general) than the one we have seen in class. The differences are:\n",
        "- In gym, transitions from a particular state and action can lead to the same destination state with different rewards (in the course all transitions reaching the same state obtain the same reward, except for transitions departing from a terminal state, whose rewards can be ignored)\n",
        "- In gym, transitions to the same state can be both terminal and non-terminal (in the course it's the states that are terminal)\n",
        "\n",
        "However we have chosen the environment `FrozenLake` which behaves as the MDPs described in the course, thus:\n",
        "- All transitions to a given state have the same reward (unless the departure and destination state is the same terminal state)\n",
        "- All transitions to a terminal state are terminal\n",
        "\n",
        "In bym `env.env.P` contains the information about the process dynamics, rewards and terminal states. The information is encoded in the following way:\n",
        "\n",
        "`env.env.P[state][action]` returns a list of tuples containing all the possible outcomes and their probability in the form `(prob, next_state, reward, terminal)` where `state` is the current state, `action` is the action taken, `probability` is the probability of the transition, `next_state` is the destination state of the transition, `reward` is the reward obtained for the transition and `terminal` is a boolean indicating if the transition ends the episode.\n",
        "\n",
        "## Exercise : MDP from gym environment\n",
        "\n",
        "Using this information compute the matrices $\\mathcal{P}_{ss'}$, $\\mathcal{R}_s$ and `terminal_states` of your MDP. Terminal is a boolean vector containing `True` for terminal states and `False` otherwise.\n",
        "\n",
        "We will use the uniform policy that we created before, that assigns equal probability to all actions.\n",
        "\n",
        "**Hint** all the transitions from a terminal state must be ignored.\n",
        "\n",
        "**Optionally** while computing the matrices verify with `assert` that the conditions that we described above are fulfilled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPg4jMnPUMWA"
      },
      "source": [
        "def compute_P_and_R(env, policy):\n",
        "    # Initialize the matrices P and R\n",
        "    n_states = env.observation_space.n\n",
        "    P = np.zeros((n_states, n_states))\n",
        "\n",
        "    R = np.empty((n_states, 1))\n",
        "    R[:] = np.nan\n",
        "\n",
        "    # Initialize a vector for terminal states\n",
        "    # we will initialize with nan\n",
        "    terminal_states = np.empty(n_states)\n",
        "    terminal_states[:] = np.nan\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # Iterate over env.env.P.items()\n",
        "    # env.env.P is a dictionary that maps from \n",
        "    # departure state to a dictionary of possible actions and subsequent transitions\n",
        "\n",
        "\n",
        "        # Iterate over all possible actions with equal probability\n",
        "\n",
        "\n",
        "            # Obtain the probability of taking that action\n",
        "\n",
        "\n",
        "            # Iterate over all transitions for this action\n",
        "\n",
        "                # If the next state is terminal flag it as such in terminal_states\n",
        "\n",
        "\n",
        "\n",
        "                # If we are not departing from a terminal state\n",
        "\n",
        "                    # Add to P the probability of the transition as:\n",
        "                    # (probability of the action) * (probability of the transition)\n",
        "\n",
        "\n",
        "                    # Set the reward in R\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "    N = env.observation_space.n\n",
        "    P = np.zeros((N,N))\n",
        "    num_of_action = env.action_space.n\n",
        "    terminal_states = np.empty(N)\n",
        "    terminal_states[:] = np.nan\n",
        "    for i in range(N):\n",
        "        state = env.env.P[i]\n",
        "        prob_to_take_a_action = 1/num_of_action\n",
        "        for action in state:\n",
        "            state_action_dynamics = state[action]\n",
        "            # print(f'state:{i}, action: {action}')\n",
        "            # print(state_action_dynamics)\n",
        "            for result in state_action_dynamics:\n",
        "                \n",
        "                next_state = result[1]\n",
        "                prob = result[0]\n",
        "                is_terminal = result[3]\n",
        "                terminal_states[next_state] = is_terminal\n",
        "                # if P[i,next_state]!=0:\n",
        "                #     print('found_something')\n",
        "                #     print(prob)\n",
        "                #     print(P[i,next_state])\n",
        "                    \n",
        "                P[i,next_state] += prob*prob_to_take_a_action\n",
        "        \n",
        "            \n",
        "\n",
        "    # Convert the terminal states vector to boolean to use in indexing\n",
        "    terminal_states = terminal_states.astype(bool)\n",
        "    R = np.zeros((N,1))\n",
        "    R[N-1,0] = 1\n",
        "\n",
        "    return P, R, terminal_states\n",
        "\n",
        "env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\"))\n",
        "P, R, terminal_states = compute_P_and_R(env, uniform_policy)\n",
        "\n",
        "print(P)\n",
        "print(R)\n",
        "print(terminal_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLQdnnJwtePi"
      },
      "source": [
        "## Exercise : Sample an episode\n",
        "\n",
        "Sample an episode with the uniform policy and return the lists of observations (states), actions, rewards and the done flags.\n",
        "\n",
        "Notice that the list of states should have one more element (the initial state).\n",
        "\n",
        "Test the function by rendering an episode. We will use `RecordVideo` to do that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery = True))\n",
        "env.reset()\n",
        "count = 0\n",
        "for i in range(100):\n",
        "    action = env.action_space.sample()\n",
        "    \n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(obs)\n",
        "    done = terminated or truncated\n",
        "    \n",
        "    if done:\n",
        "        count+=1\n",
        "        env.reset()\n",
        "\n",
        "print(count)\n",
        "display(env.video())"
      ],
      "metadata": {
        "id": "Cd01J-VfWi3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU--ZQEdtdqF"
      },
      "source": [
        "def sample_episode(env, policy, reset=True):\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    dones = []\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # print('i am here')\n",
        "    # If reset, we reset the environment and get an initial state\n",
        "    # else we set the initial state to it's current state env.env.s\n",
        "    if reset:\n",
        "        # print('i am here hehe')\n",
        "        initial_state,_ = env.reset()\n",
        "        # print('done')\n",
        "        # print(initial_state)\n",
        "    else:\n",
        "        initial_state = env.env.s\n",
        "        # print('i am also here hehe')\n",
        "\n",
        "    done = False\n",
        "    # Collect the initial state\n",
        "    states.append(initial_state)\n",
        "    \n",
        "    # While the episode has not finished\n",
        "    while not done:\n",
        "        \n",
        "        # Select an action\n",
        "        action = policy.sample(states[-1])\n",
        "        actions.append(action)\n",
        "        \n",
        "        # Step the environment\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        \n",
        "        # The episode is done if it has been terminated or truncated\n",
        "        done = terminated or truncated \n",
        "\n",
        "        # print(obs)\n",
        "        # Collect the state, reward and action taken\n",
        "        states.append(obs)\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return states, actions, rewards, dones\n",
        "\n",
        "env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\"))\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "states, actions, rewards, dones = sample_episode(env, uniform_policy)\n",
        "print(rewards)\n",
        "print(len(states), len(actions), len(rewards), len(dones))\n",
        "# print(states)\n",
        "display(env.video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIn7ouMBPVhW"
      },
      "source": [
        "## Exercise : Returns of episode\n",
        "\n",
        "For a sampled episode compute the return $G_t$ for all steps $t$.\n",
        "\n",
        "**Hint** the return is easily computed backwards from the last step in the episode to the first."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_returns(rewards, gamma):\n",
        "    returns = np.zeros(len(rewards))\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    #changed this\n",
        "    # rewards = np.array(rewards)/len(rewards)\n",
        "    # Iterate over the rewards backward computing each return\n",
        "    \n",
        "    # using the previous return computed\n",
        "    episode_len = len(rewards)\n",
        "    # print(f\"before return {returns}, {episode_len}, {rewards[episode_len -1]}\")\n",
        "    returns[episode_len-1] = rewards[episode_len-1]\n",
        "    # print(f\"first return {returns}\")\n",
        "    \n",
        "    for i,reward in enumerate(reversed(rewards)):\n",
        "        if i==0:\n",
        "            # print(f\"{episode_len-1-i} and {rewards[episode_len-1-i]} and {gamma*rewards[episode_len-i-1]}, {returns[episode_len-1-i]}\") \n",
        "            continue\n",
        "\n",
        "        returns[episode_len-1-i] = rewards[episode_len-1-i] + gamma*returns[episode_len-i]\n",
        "        # print(f\"{episode_len-1-i} and {rewards[episode_len-1-i]} and {gamma*returns[episode_len-i]}, {returns[episode_len-1-i]}\")\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return returns\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "\n",
        "# Now we will run until one of them has a positive reward\n",
        "while True:\n",
        "    states, actions, rewards, dones = sample_episode(env, uniform_policy)\n",
        "\n",
        "    if np.sum(rewards) > 0:\n",
        "        # Print the rewards\n",
        "        print(rewards)\n",
        "\n",
        "        # Compute and print the returns\n",
        "        gamma = 0.9\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        print(returns)\n",
        "\n",
        "        # Exit the loop\n",
        "        break"
      ],
      "metadata": {
        "id": "L4QBVOOaLAup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy evaluation\n",
        "\n",
        "In this section we will implement various value function estimation methods:\n",
        "- Direct method using the previous P and R\n",
        "- Naive (cheating) method that assumes possibility of setting an arbitrary initial state\n",
        "- Monte Carlo methods\n",
        "- Time-difference method"
      ],
      "metadata": {
        "id": "2ZKleEtAaCzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : Direct method\n",
        "\n",
        "Solve the Bellman equation, using the direct solution (matrix inversion).\n",
        "\n",
        "**Note** that this method leads to miss-leading values for terminal states, since there is no way to indicate terminality using the Bellman equation without sampling."
      ],
      "metadata": {
        "id": "MQWcUVBRZuCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_direct(env, gamma, policy):\n",
        "    P, R, terminal_states = compute_P_and_R(env, policy)\n",
        "    P[env.observation_space.n-1, env.observation_space.n-1] = 0 \n",
        "    ### BEGIN SOLUTION\n",
        "    I = np.eye(env.observation_space.n)\n",
        "    v = np.linalg.inv(I-gamma*P)@R   \n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    # Unsqueeze the extra dimension\n",
        "    v = v[:,0]\n",
        "\n",
        "    return v\n",
        "    \n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "v_direct = value_direct(env, gamma, uniform_policy)\n",
        "print(v_direct)"
      ],
      "metadata": {
        "id": "Ez1vA5iHX88p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Exercise : Verify the solution\n",
        "\n",
        "Check if the computed value function $V$ fulfills the Bellman equation. You may use `np.allclose(a, b)` to check if all elements in `a` and `b` are close up to a numerical error."
      ],
      "metadata": {
        "id": "z-uYC9r7b8FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "qBPRJO3R99jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the following we must set the value function of terminal states to 0 in order to keep our value function comparable to those computed by methods using episode sampling."
      ],
      "metadata": {
        "id": "rsky-dU1cb07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To compare to the following methods we must set the value of terminal states to 0\n",
        "v_direct[terminal_states] = 0\n",
        "print(v_direct)"
      ],
      "metadata": {
        "id": "ht_IqbVaUtM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : Naive (cheating) method\n",
        "\n",
        "Compute the value function for each state by cheating (changing the starting state and computing the average return from the start)."
      ],
      "metadata": {
        "id": "A9E3mBp3ZHdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_naive(env, gamma, policy, n_episodes):\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    # Initialize values and counts tables (one cell per state)\n",
        "    values = np.zeros((env.observation_space.n,1))\n",
        "    counts = np.zeros((env.observation_space.n,1))\n",
        "\n",
        "\n",
        "    # Compute the number of episodes per state\n",
        "    episodes_per_state = int(n_episodes/env.observation_space.n)\n",
        "\n",
        "\n",
        "    # For each initial state\n",
        "    for i in range(env.observation_space.n):\n",
        "\n",
        "        # For each episode\n",
        "        for episode in range(episodes_per_state):\n",
        "            # Reset the environment\n",
        "            env.reset()\n",
        "\n",
        "            # Set the initial state\n",
        "            env.env.s = i\n",
        "\n",
        "            # Sample an episode \n",
        "            # (without reseting the environment to avoid changing the initial state)\n",
        "            states, actions, rewards, dones = sample_episode(env, policy, reset = False)\n",
        "\n",
        "            # Compute the returns\n",
        "            # gamma = 0.9\n",
        "            returns = compute_returns(rewards, gamma)\n",
        "            \n",
        "            # Accumulate the return of the initial state\n",
        "            values[i] += returns[0]\n",
        "\n",
        "        # Divide the accumulated value by the number of episodes\n",
        "        values[i] = values[i]/episodes_per_state\n",
        "\n",
        "    ### END SOLUTION\n",
        "    \n",
        "    return values\n",
        "\n",
        "def compute_value_error(v_est, v_ref):\n",
        "    diff = (v_est - v_ref)\n",
        "    return np.mean(diff), np.std(diff)\n",
        "\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "n_episodes = 10000\n",
        "v_naive = value_naive(env, gamma, uniform_policy, n_episodes)\n",
        "print(v_naive)\n",
        "print(compute_value_error(v_naive, v_direct))"
      ],
      "metadata": {
        "id": "Ld7tEKgpbTDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7q9lKzVoDD"
      },
      "source": [
        "## Exercise : Monte Carlo first visit method\n",
        "Compute the value function for each state using the Monte carlo method \"first visit\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def value_montecarlo_first(env, gamma, policy, n_episodes):\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    # Initialize values and counts tables (one cell per state)\n",
        "    values = np.zeros((env.observation_space.n,1))\n",
        "    counts = np.zeros((env.observation_space.n,1))\n",
        "\n",
        "    # For each episode\n",
        "    for episode in range(n_episodes):\n",
        "        # Sample an episode and compute returns\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        # print(states)\n",
        "        # Keep track of visited states\n",
        "        visited_states = set(states)\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "\n",
        "        # For each state and associated return\n",
        "        for state, ret in zip(states, returns):\n",
        "            # If first visit\n",
        "            if state in visited_states:\n",
        "                \n",
        "                # Increment counts\n",
        "                counts[state]+=1\n",
        "\n",
        "                # Accumulate returns\n",
        "                values[state]+=ret \n",
        "\n",
        "                # Update the set of visited states\n",
        "                visited_states.remove(state)\n",
        "\n",
        "    # Average the accumulated returns\n",
        "    for i in range(env.observation_space.n):\n",
        "        if counts[i]!=0:\n",
        "            values[i] = values[i]/counts[i]\n",
        "\n",
        "    ### END SOLUTION\n",
        "    \n",
        "    return values\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "n_episodes = 10000\n",
        "v_mc_firstvisit = value_montecarlo_first(env, gamma, uniform_policy, n_episodes)\n",
        "print(v_mc_firstvisit)\n",
        "print(compute_value_error(v_mc_firstvisit, v_direct))"
      ],
      "metadata": {
        "id": "VuqTZ7-YwAhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKe-5izX3NXr"
      },
      "source": [
        "## Exercise : Monte Carlo every visit method\n",
        "Compute the value function for each state using the Monte carlo method \"every visit\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1quaaWpt3MwK"
      },
      "source": [
        "def value_montecarlo_every(env, gamma, policy, n_episodes):\n",
        "    ### BEGIN SOLUTION\n",
        "    # Initialize values and counts tables (one cell per state)\n",
        "    values = np.zeros((env.observation_space.n,1))\n",
        "    counts = np.zeros((env.observation_space.n,1))\n",
        "\n",
        "    # For each episode\n",
        "    for episode in range(n_episodes):\n",
        "        # Sample an episode and compute returns\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "\n",
        "        # For each state and associated return\n",
        "        for state, ret in zip(states, returns):\n",
        "            # Increment counts\n",
        "            counts[state]+=1\n",
        "\n",
        "            # Accumulate returns\n",
        "            values[state]+=ret \n",
        "\n",
        "    # Average the accumulated returns\n",
        "    for i in range(env.observation_space.n):\n",
        "        if counts[i]!=0:\n",
        "            values[i] = values[i]/counts[i]\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "    \n",
        "    return values\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "n_episodes = 10000\n",
        "v_mc_everyvisit = value_montecarlo_every(env, gamma, uniform_policy, n_episodes)\n",
        "print(v_mc_everyvisit)\n",
        "print(compute_value_error(v_mc_everyvisit, v_direct))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : Monte Carlo incremental\n",
        "\n",
        "Implement the incremental Monte Carlo method."
      ],
      "metadata": {
        "id": "oTxeLk1BiTAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_montecarlo_incremental(env, gamma, policy, n_episodes):\n",
        "    ### BEGIN SOLUTION\n",
        "    # Initialize values and counts tables (one cell per state)\n",
        "\n",
        "    values = np.zeros((env.observation_space.n,1))\n",
        "    counts = np.zeros((env.observation_space.n,1))\n",
        "\n",
        "\n",
        "\n",
        "    # For each episode\n",
        "    for episode in range(n_episodes):\n",
        "    \n",
        "        # Sample an episode and compute returns\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "\n",
        "        # For each state and associated return\n",
        "        for state, ret in zip(states, returns):\n",
        "        \n",
        "            # Increment counts\n",
        "            counts[state]+=1\n",
        "\n",
        "            # Update value with return\n",
        "            values[state] = values[state]+ (ret-values[state])/counts[state]\n",
        "\n",
        "    ### END SOLUTION\n",
        "    return values\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "n_episodes = 10000\n",
        "v_mc_incremental = value_montecarlo_incremental(env, gamma, uniform_policy, n_episodes)\n",
        "print(v_mc_incremental)\n",
        "print(compute_value_error(v_mc_incremental, v_direct))"
      ],
      "metadata": {
        "id": "YMe7VelViSlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : Time-differences (TD) method"
      ],
      "metadata": {
        "id": "-5oibABvkhSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "def value_td(env, gamma, policy, n_episodes, alpha=0.4):\n",
        "    ### BEGIN SOLUTION\n",
        "    # Initialize value table (one cell per state)\n",
        "    values = np.zeros((env.observation_space.n,1))\n",
        "\n",
        "\n",
        "    # For each episode\n",
        "    for episode in range(n_episodes):\n",
        "        # Sample an episode\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        # print(len(states))\n",
        "        # print(len(rewards))\n",
        "        # For each step in the episode\n",
        "        for i,state in enumerate(states):\n",
        "            # Update the value of the depart state with the current value, the value of the next state and the reward\n",
        "            # print(i)\n",
        "            if i<len(rewards):\n",
        "                values[state] = values[state] + alpha*(rewards[i] + gamma*values[states[i+1]] - values[state])\n",
        "        \n",
        "        ### END SOLUTION\n",
        "    return values\n",
        "\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "n_episodes = 10000\n",
        "alpha = 0.4\n",
        "v_td = value_td(env, gamma, uniform_policy, n_episodes, alpha=alpha)\n",
        "print(v_td)\n",
        "print(compute_value_error(v_td, v_direct))"
      ],
      "metadata": {
        "id": "ydLYFgJTkpdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Exercise : Comparison TD to Monte Carlo incremental\n",
        "\n",
        "How do these to methods compare? Explain the relative advantages and disadvantages."
      ],
      "metadata": {
        "id": "GOej2Xz9fW9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cQBdi02Kfn-s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9VPqxta4fjV"
      },
      "source": [
        "## Exercise : Action-value\n",
        "Compute the action value function using one or more of the following methods:\n",
        "- Naive (cheating)\n",
        "- MC \"first visit\"\n",
        "- MC \"every visit\"\n",
        "- MC incremental\n",
        "- TD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaM-a7XE4fG1"
      },
      "source": [
        "def actionvalue_montecarlo_every(env, gamma, policy, n_episodes):\n",
        "    ### BEGIN SOLUTION\n",
        "    q = np.zeros((env.observation_space.n,  env.action_space.n))\n",
        "    counts = np.zeros((env.observation_space.n,  env.action_space.n))\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "\n",
        "        for state, action, ret in zip(states, actions, returns):\n",
        "            # Increment counts\n",
        "            counts[state,action]+=1\n",
        "\n",
        "            # Accumulate returns\n",
        "            q[state, action]+=ret \n",
        "\n",
        "    # Average the accumulated returns\n",
        "    for i in range(env.observation_space.n):\n",
        "        for j in range(env.action_space.n):\n",
        "            if counts[i,j]!=0:\n",
        "                q[i,j] = q[i,j]/counts[i,j]\n",
        "                # values[i] = values[i]/counts[i]\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "    return q\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "gamma = 0.9\n",
        "n_episodes = 10000*5\n",
        "q_mc_everyvisit = actionvalue_montecarlo_every(env, gamma, uniform_policy, n_episodes)\n",
        "print(q_mc_everyvisit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy improvement\n",
        "\n",
        "In this section we are going to improve upon the uniform policy, which selects actions at random, independently on the state.\n",
        "\n",
        "To assess whether our learned policies work, we will start by implmenting a scoring function to evaluate the policies."
      ],
      "metadata": {
        "id": "zwTMi1jvhI9G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9lI0QkV9kA"
      },
      "source": [
        "## Exercise : Evaluate performance of a policy\n",
        "\n",
        "In this environment we define a goal as obtaining a final reward greater than 0, thus reaching the target state, since it is the only one with a non-zero reward.\n",
        "\n",
        "For a given policy, compute:\n",
        "- the average number of episodes where the goal was reached\n",
        "- the average number of steps to reach the goal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cthBcdO5eHaC"
      },
      "source": [
        "def score_policy(env, gamma, policy, n_episodes):\n",
        "    episodes_to_goal = 0\n",
        "    steps_to_goal = []\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    for episode in range(n_episodes):\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        if rewards[-1]!=0:\n",
        "            episodes_to_goal+=1\n",
        "            steps_to_goal.append(len(rewards))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return episodes_to_goal/n_episodes, np.mean(steps_to_goal)\n",
        "\n",
        "score_policy(env, 0.9, uniform_policy, 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUNB0Ssg0zWO"
      },
      "source": [
        "## Exercise : Greedy policy (policy improvement)\n",
        "\n",
        "Create a greedy policy from an action value function and evaluate it's performance.\n",
        "\n",
        "The greedy policy selects the action leading to the largest value in the current state.\n",
        "\n",
        "**Note** that when several actions have the same maximal value, it is best to randomly pick one of them.\n",
        "\n",
        "Pick the first highest value action or (optionally) pick randomly amongst actions with the highest value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-pHndGx18rX"
      },
      "source": [
        "class GreedyPolicy(UniformPolicy):\n",
        "    def __init__(self, action_space, q):\n",
        "        super().__init__(action_space)\n",
        "\n",
        "        # q is the action-value table\n",
        "        self.q = q\n",
        "\n",
        "    def _max_value_action(self, state):\n",
        "        ### BEGIN SOLUTION\n",
        "\n",
        "        action = np.argmax(self.q[state])\n",
        "        ### END SOLUTION\n",
        "        return action\n",
        "\n",
        "    def probability(self, state, action):\n",
        "        # Select the highest value action\n",
        "        action_max = self._max_value_action(state)\n",
        "\n",
        "        # Return a probability of 1 for the selected action 0 otherwise\n",
        "        action_prob = float(action == action_max)\n",
        "        \n",
        "        return action_prob\n",
        "\n",
        "    def sample(self, state):\n",
        "        # Select the highest value action\n",
        "        action_max = self._max_value_action(state)\n",
        "\n",
        "        return action_max\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "greedy_policy = GreedyPolicy(env.action_space, q_mc_everyvisit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3rHncCg2DyX"
      },
      "source": [
        "## Exercise : Compare the performance of policies\n",
        "\n",
        "Report the performance of the uniform policy and the greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCGNWhq83GkV"
      },
      "source": [
        "# env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery = True))\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "un_per,un_mean =  score_policy(env, 0.9, uniform_policy, 10000)\n",
        "print(f\"uniform policy: {un_per:.2%} / {un_mean}\")\n",
        "\n",
        "gr_per,gr_mean = score_policy(env, 0.9, greedy_policy, 10000)\n",
        "print(f\"greedy policy: {gr_per:.2%} / {gr_mean}\")\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env.render_mode = \"rgb_array\"\n",
        "env = RecordVideo(gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery = True))\n",
        "for i in range(10):\n",
        "    states, actions, rewards, dones = sample_episode(env, greedy_policy)\n",
        "display(env.video())"
      ],
      "metadata": {
        "id": "9taz0Npfu-go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy learning\n",
        "\n",
        "In this section we will start learning policies.\n",
        "\n",
        "We will then implement the following policy learning methods:\n",
        "- policy iteration\n",
        "- SARSA\n",
        "- Q-Learning"
      ],
      "metadata": {
        "id": "IL_UQM1yhPkA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-qvM1ceIGm"
      },
      "source": [
        "## Exercise : Policy iteration\n",
        "By alternating between policy evaluation and policy improvement, find an optimal policy.\n",
        "\n",
        "Print the scores of each intermediate policy and comment on how the metrics evolve."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial policy: 1.80% / 14.28\n",
        "# Policy (iter: 0): 2.80% / 12.54, Value change: nan\n",
        "# Policy (iter: 1): 3.20% / 12.00, Value change: 2.97\n",
        "# Policy (iter: 2): 2.70% / 11.52, Value change: 2.46\n",
        "# Policy (iter: 3): 3.40% / 12.38, Value change: 2.12\n",
        "# Policy (iter: 4): 6.00% / 15.42, Value change: 4.32\n",
        "# Policy (iter: 5): 6.60% / 14.41, Value change: 4.05\n",
        "# Policy (iter: 6): 7.50% / 15.71, Value change: 0.36\n",
        "# Policy (iter: 7): 7.00% / 16.91, Value change: 0.84\n",
        "# Policy (iter: 8): 7.00% / 13.84, Value change: 0.88\n",
        "# Policy (iter: 9): 7.70% / 16.21, Value change: 0.45\n",
        "# Policy (iter: 10): 6.00% / 15.52, Value change: 0.44\n",
        "# Policy (iter: 11): 6.20% / 15.35, Value change: 0.62\n",
        "# Policy (iter: 12): 7.10% / 15.46, Value change: 0.98\n",
        "# Policy (iter: 13): 8.10% / 15.41, Value change: 0.74\n",
        "# Policy (iter: 14): 8.10% / 16.12, Value change: 0.43\n",
        "# Policy (iter: 15): 9.00% / 15.02, Value change: 0.60\n",
        "# Policy (iter: 16): 7.60% / 16.47, Value change: 0.60\n",
        "# Policy (iter: 17): 7.20% / 16.10, Value change: 0.58\n",
        "# Policy (iter: 18): 6.80% / 16.54, Value change: 0.52\n",
        "# Policy (iter: 19): 8.80% / 17.15, Value change: 0.88\n",
        "# Policy (iter: 20): 7.20% / 16.44, Value change: 0.45\n",
        "# Policy (iter: 21): 5.60% / 11.82, Value change: 0.48\n",
        "# Policy (iter: 22): 6.80% / 12.63, Value change: 1.43\n",
        "# Policy (iter: 23): 5.00% / 11.94, Value change: 1.30\n",
        "# Policy (iter: 24): 3.80% / 14.11, Value change: 0.51\n",
        "# Policy (iter: 25): 7.90% / 15.90, Value change: 2.13\n",
        "# Policy (iter: 26): 11.80% / 16.68, Value change: 2.09\n",
        "# Policy (iter: 27): 9.50% / 17.68, Value change: 0.82\n",
        "# Policy (iter: 28): 8.90% / 16.01, Value change: 0.27\n",
        "# Policy (iter: 29): 9.30% / 15.66, Value change: 1.02"
      ],
      "metadata": {
        "id": "Xyl8iKSGhB_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0BBrHPoeZjg"
      },
      "source": [
        "def policy_learn_iteration(env, initial_policy, policy_evaluation_function, \n",
        "                           n_episodes_value, n_episodes_score, n_iterations):\n",
        "    policy = initial_policy\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    # Score and print the initial policy\n",
        "    per,mean = score_policy(env, 0.9, policy, n_episodes_score)\n",
        "    print(f\"Initial Policy: {per:.2%} / {mean}\")\n",
        "    action_value_prev = None\n",
        "        # Policy evaluation\n",
        "    for it in range(n_iterations):\n",
        "        if it==0:\n",
        "            per,mean = score_policy(env, 0.9, policy, n_episodes_score)\n",
        "            print(f\"Policy (iter: {it}): {per:.2%} / {mean:.4}, Value change: nan\")\n",
        "            \n",
        "\n",
        "        # Keep track of the action-value function change \n",
        "        # (sum of absolute difference between the previous \n",
        "        #  and next action-value funcitons)\n",
        "        # Note that at the first step we will report a change of nan\n",
        "        # since we still don't have a previous action-value function\n",
        "        action_value = policy_evaluation_function(env, gamma, policy, n_episodes_value)\n",
        "        if it!=0:\n",
        "            diff = np.abs(np.diff(action_value_prev - action_value))\n",
        "            \n",
        "        action_value_prev = action_value\n",
        "        \n",
        "        # Policy improvement\n",
        "        policy = GreedyPolicy(env.action_space, action_value)\n",
        "\n",
        "        # Policy scoring\n",
        "        if it!=0:\n",
        "            \n",
        "            per,mean = score_policy(env, 0.9, policy, n_episodes_score)\n",
        "            print(f\"Policy (iter: {it}): {per:.2%} / {mean:.4}, Value change: {np.sum(diff):.3}\")\n",
        "            \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "uniform_policy = UniformPolicy(env.action_space)\n",
        "\n",
        "policy_learn_iteration(env, uniform_policy, actionvalue_montecarlo_every, \n",
        "                       n_episodes_value=1000, n_episodes_score=10000,\n",
        "                       n_iterations=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : Epsilon-greedy policy\n",
        "\n",
        "One of the issues with the greedy policy is that some possible trajectories may never be visited depending on the initial estimation of the value function.\n",
        "\n",
        "To avoid this create an epsilon-greedy policy. These policies act differently when running in training mode and evaluation mode. In evaluation mode they act as a greedy policy. In training mode:\n",
        "- with probability epsilon, uniformly selects an action\n",
        "- else selects the action with maximum value (greedy policy)\n",
        "\n",
        "Implement the `EpsilonGreedyPolicy` class.\n"
      ],
      "metadata": {
        "id": "QndS7WXsg6WW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZ1iOADLl8vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyPolicy(GreedyPolicy):\n",
        "    def __init__(self, action_space, q, epsilon, \n",
        "                 epsilon_decay=1, epsilon_min=0):\n",
        "        super().__init__(action_space, q)\n",
        "        self.epsilon_start = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        self.epsilon = self.epsilon_start\n",
        "\n",
        "    def sample(self, state):\n",
        "        ### BEGIN SOLUTION\n",
        "        if np.random.random()<= self.epsilon and self.training:\n",
        "            choices = [i for i in range(self.n_actions)]\n",
        "            action = np.random.choice(choices)\n",
        "        else:\n",
        "            action = np.argmax(self.q[state])\n",
        "\n",
        "\n",
        "\n",
        "        ### END SOLUTION\n",
        "        return action\n",
        "\n",
        "    def begin_episode(self, episode_index):\n",
        "        # Start of an episode\n",
        "        self.epsilon = self.epsilon_start * (self.epsilon_decay ** episode_index)\n",
        "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "# Instantiate a policy\n",
        "dummy = EpsilonGreedyPolicy(env.action_space, q_mc_everyvisit, epsilon=0.5)\n",
        "\n",
        "# Sample 20 actions from state 0 in train mode\n",
        "dummy.train()\n",
        "actions = [dummy.sample(2) for i in range(20)]\n",
        "print(actions)\n",
        "\n",
        "# Sample 20 actions from state 0 in eval mode\n",
        "dummy.eval()\n",
        "actions = [dummy.sample(2) for i in range(20)]\n",
        "print(actions)"
      ],
      "metadata": {
        "id": "AFTZYwe0g9j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : SARSA\n",
        "\n",
        "State–action–reward–state–action (SARSA) is a method to learn a policy leveraging the TD estimation of action-value functions and an epsilon-greedy policy.\n",
        "\n",
        "Since it is based on TD, it does not require full episodes to train, the policy can be improved at each step. We will therefore **not use the `sample_episode`** and reimplement here a similar loop.\n",
        "\n",
        "Implement the SARSA algorithm, in this case it will be passed a policy of type `EpsilonGreedyPolicy` which stores the action-value (or Q-table). We may access and modify if using `policy.q`.\n",
        "\n",
        "Score the trained policy."
      ],
      "metadata": {
        "id": "V_8lWjC7tyFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_learn_sarsa(env, policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500):\n",
        "    # No need to create the tables of action-values since they are stored directly in the policy\n",
        "    policy.train()\n",
        "    # While we haven't reached the desired number of steps\n",
        "    for episode in range(n_episodes):\n",
        "        # Call the policy begin_episode so it can handle epsilon decay\n",
        "        policy.begin_episode(episode)\n",
        "        \n",
        "        # Print every couple episodes\n",
        "        if not episode % print_every:\n",
        "            print(f'ep: {episode}, epsilon: {policy.epsilon:.3f}')\n",
        "            policy.eval()\n",
        "            avg_episodes_to_goal,avg_steps_to_goal = score_policy(env, gamma, policy, 10000)\n",
        "            print(f'Policy SARSA: {avg_episodes_to_goal:.2%} / {avg_steps_to_goal:.2f}')\n",
        "            policy.train()\n",
        "            print()\n",
        "        # Get initial state\n",
        "        state, info = env.reset()\n",
        "\n",
        "        # Take the first action acording to the policy\n",
        "        action = policy.sample(state)\n",
        "\n",
        "        # While we haven't reached the maximum number of steps for the episode\n",
        "        for step in range(max_n_steps):     \n",
        "            ### BEGIN SOLUTION\n",
        "            # Perform a step of the environment\n",
        "            state_next,reward,terminated,truncated,info =  env.step(action)\n",
        "\n",
        "            # It is done if terminated or truncated\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # If episode has finished\n",
        "            if done:\n",
        "                # Update the action-value table and leave the loop\n",
        "                action_next = policy.sample(state_next)\n",
        "                policy.q[state, action] += alpha * (reward + gamma * policy.q[state_next, action_next] - policy.q[state, action])\n",
        "                  \n",
        "                break\n",
        "\n",
        "            # Sample the next action\n",
        "            action_next = policy.sample(state_next)\n",
        "\n",
        "            # Update the action-value table\n",
        "            policy.q[state, action] += alpha * (reward + gamma * policy.q[state_next, action_next] - policy.q[state, action])\n",
        "            ### END SOLUTION\n",
        "\n",
        "            # Set the current state and action\n",
        "            state = state_next\n",
        "            action = action_next\n",
        "            \n",
        "    return\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "\n",
        "q_initial = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "sarsa_policy = EpsilonGreedyPolicy(env.action_space, q_initial,\n",
        "                                   epsilon=1, epsilon_decay=0.999, epsilon_min=0.001)\n",
        "\n",
        "gamma = 0.9\n",
        "n_episodes = 10000\n",
        "alpha = 0.2\n",
        "n_episodes_score = 10000\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Train the policy\n",
        "sarsa_policy.train()\n",
        "policy_learn_sarsa(env, sarsa_policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500)\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Evaluate the policy\n",
        "sarsa_policy.eval()\n",
        "avg_episodes_to_goal,avg_steps_to_goal = score_policy(env, gamma, sarsa_policy, n_episodes_score)\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(f'Policy SARSA: {avg_episodes_to_goal:.2%} / {avg_steps_to_goal:.2f}')"
      ],
      "metadata": {
        "id": "2BONRb7aSTbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ep: 0, epsilon: 1.000\n",
        "# ep: 500, epsilon: 0.606\n",
        "# ep: 1000, epsilon: 0.368\n",
        "# ep: 1500, epsilon: 0.223\n",
        "# ep: 2000, epsilon: 0.135\n",
        "# ep: 2500, epsilon: 0.082\n",
        "# ep: 3000, epsilon: 0.050\n",
        "# ep: 3500, epsilon: 0.030\n",
        "# ep: 4000, epsilon: 0.018\n",
        "# ep: 4500, epsilon: 0.011\n",
        "# Policy SARSA: 11.40% / 24.82"
      ],
      "metadata": {
        "id": "rwTFYsQG5EBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise : Q-learning\n",
        "\n",
        "Q-learning is very similar to SARSA. **SARSA is an on-policy learning method** in which the action-values are updated following the same policy. In SARSA the action-values are updated using the value of the next state and next action taken.\n",
        "\n",
        "Q-learning is off-policy, it does not assume the same policy when updating the action-values, instead it assumes an optimal policy by using the maximum value of the next state.\n",
        "\n",
        "Implement a modified version of `policy_learn_sarsa` that performs Q-learning."
      ],
      "metadata": {
        "id": "qL6xBo8Lg-gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_learn_qlearn(env, policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500):\n",
        "    # No need to create the tables of action-values since they are stored directly in the policy\n",
        "    policy.train()\n",
        "    # While we haven't reached the desired number of steps\n",
        "    for episode in range(n_episodes):\n",
        "        # Call the policy begin_episode so it can handle epsilon decay\n",
        "        policy.begin_episode(episode)\n",
        "        \n",
        "        # Print every couple episodes\n",
        "        if not episode % print_every:\n",
        "            print(f'ep: {episode}, epsilon: {policy.epsilon:.3f}')\n",
        "            policy.eval()\n",
        "            avg_episodes_to_goal,mean = score_policy(env, gamma, policy, 100)\n",
        "            print(f'Policy Q-learn: {avg_episodes_to_goal}, mean\"{mean}')\n",
        "            print()\n",
        "            policy.train()\n",
        "        # Get initial state\n",
        "        state, info = env.reset()\n",
        "\n",
        "        # Take the first action acording to the policy\n",
        "        action = policy.sample(state)\n",
        "\n",
        "        # While we haven't reached the maximum number of steps for the episode\n",
        "        for step in range(max_n_steps):     \n",
        "            ### BEGIN SOLUTION\n",
        "            # Perform a step of the environment\n",
        "            state_next,reward,terminated,truncated,info =  env.step(action)\n",
        "\n",
        "            # It is done if terminated or truncated\n",
        "            done = terminated or truncated\n",
        "\n",
        "\n",
        "            # If episode has finished\n",
        "            if done:\n",
        "                # Update the action-value table and leave the loop\n",
        "                policy.q[state, action] += alpha*(reward + gamma*np.max(policy.q[state_next]) - policy.q[state,action]) \n",
        "                break\n",
        "\n",
        "            # Sample the next action\n",
        "            action_next = policy.sample(state_next)\n",
        "\n",
        "            # Update the action-value table\n",
        "            policy.q[state, action] += alpha*(reward + gamma*np.max(policy.q[state_next]) - policy.q[state,action])\n",
        "            ### END SOLUTION\n",
        "\n",
        "            # Set the current state and action\n",
        "            state = state_next\n",
        "            action = action_next\n",
        "            \n",
        "    return\n",
        "\n",
        "# env = gym.make(\"FrozenLake-v1\", render_mode=None)\n",
        "\n",
        "# q_initial = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "# qlearn_policy = EpsilonGreedyPolicy(env.action_space, q_initial,\n",
        "#                                     epsilon=1, epsilon_decay=0.999, epsilon_min=0.01)\n",
        "\n",
        "# gamma = 0.9\n",
        "# n_episodes = 5000\n",
        "# alpha = 0.2\n",
        "# n_episodes_score = 1000\n",
        "\n",
        "# ### BEGIN SOLUTION\n",
        "# # Train the policy\n",
        "# qlearn_policy.train()\n",
        "# policy_learn_qlearn(env, qlearn_policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500)\n",
        "\n",
        "# ### END SOLUTION\n",
        "\n",
        "# ### BEGIN SOLUTION\n",
        "# # Evaluate the policy\n",
        "# qlearn_policy.eval()\n",
        "# avg_episodes_to_goal,avg_steps_to_goal = score_policy(env, gamma, qlearn_policy, n_episodes_score)\n",
        "\n",
        "\n",
        "# ### END SOLUTION\n",
        "\n",
        "# print(f'Policy Q-learn: {avg_episodes_to_goal:.2%} / {avg_steps_to_goal:.2f}')\n"
      ],
      "metadata": {
        "id": "oGIZgia8hA--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ep: 0, epsilon: 1.000\n",
        "# ep: 500, epsilon: 0.606\n",
        "# ep: 1000, epsilon: 0.368\n",
        "# ep: 1500, epsilon: 0.223\n",
        "# ep: 2000, epsilon: 0.135\n",
        "# ep: 2500, epsilon: 0.082\n",
        "# ep: 3000, epsilon: 0.050\n",
        "# ep: 3500, epsilon: 0.030\n",
        "# ep: 4000, epsilon: 0.018\n",
        "# ep: 4500, epsilon: 0.011\n",
        "# Policy Q-learn: 50.50% / 30.29"
      ],
      "metadata": {
        "id": "eR-tSlE6O_1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Exercise : Train for Taxi\n",
        "\n",
        "Train an epsilon-greedy policy using Q-learning on the `Taxi-v3` environment.\n",
        "\n",
        "Score the performance of this policy and compare it to a uniform policy."
      ],
      "metadata": {
        "id": "RLlMIlu-XcwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
        "\n",
        "q_initial = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "qlearn_policy = EpsilonGreedyPolicy(env.action_space, q_initial,\n",
        "                                    epsilon=1, epsilon_decay=0.999, epsilon_min=0.01)\n",
        "\n",
        "gamma = 0.9\n",
        "n_episodes = 5000\n",
        "alpha = 0.2\n",
        "n_episodes_score = 1000\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Train the policy\n",
        "qlearn_policy.train()\n",
        "policy_learn_qlearn(env, qlearn_policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500)\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Evaluate the policy\n",
        "qlearn_policy.eval()\n",
        "avg_episodes_to_goal,avg_steps_to_goal = score_policy(env, gamma, qlearn_policy, n_episodes_score)\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(f'Policy Q-learn: {avg_episodes_to_goal:.2%} / {avg_steps_to_goal:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "FFFmwuiC6Odi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Exercise : Render Taxi\n",
        "\n",
        "Run 3 episodes of a Q-learn trained policy on `Taxi-v3` this time rendering the result."
      ],
      "metadata": {
        "id": "zQcpdmK5X6tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
        "env = RecordVideo(gym.make(\"Taxi-v3\", render_mode=\"rgb_array\"))\n",
        "qlearn_policy.eval()\n",
        "for i in range(5):\n",
        "    states, actions, rewards, dones = sample_episode(env, qlearn_policy)\n",
        "display(env.video())\n",
        "\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "c8WtbxYmXxoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Bonus) Exercise : A observation space environment\n",
        "\n",
        "Try to perform Q-learning on an environment (e.g. `CartPole-v1`) with continuous action and observation spaces.\n",
        "\n",
        "You will need to discretize the observation space."
      ],
      "metadata": {
        "id": "u6ouIi6klkt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of rendering and showing a CartPole-v1 environment\n",
        "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"))\n",
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "\n",
        "observation, info = env.reset()\n",
        "i=0\n",
        "while True:\n",
        "    env.render()\n",
        "    \n",
        "    action = env.action_space.sample() \n",
        "    state_next, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    \n",
        "    if done: \n",
        "      break\n",
        "            \n",
        "env.close()"
      ],
      "metadata": {
        "id": "-ktToiKRZPS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# class ObservationWrapper(gym.ObservationWrapper):\n",
        "#     def __init__(self, env, n_states=4096, bounds=[(-4.8,4.8),(-50,50),( -0.418, 0.418),(-20,20)]):\n",
        "#         super().__init__(env)\n",
        "\n",
        "#         self.n_states = n_states\n",
        "\n",
        "#         assert isinstance(env.observation_space, gym.spaces.box.Box)\n",
        "\n",
        "#         ### BEGIN SOLUTION\n",
        "#         # Get the dimensions of the observation space\n",
        "#         self.dimension = env.observation_space.shape[0]\n",
        "#         self.states_per_dimention = n_states**0.25\n",
        "\n",
        "#         self.cart_pos_bound = bounds[0]\n",
        "#         self.cart_vel_bound = bounds[1]\n",
        "#         self.pole_angle_bound = bounds[2]\n",
        "#         self.pole_angleVel_bound = bounds[3]\n",
        "\n",
        "        \n",
        "#         self.cart_pos_step = (self.cart_pos_bound[1]- self.cart_pos_bound[0])/self.states_per_dimention\n",
        "#         self.cart_vel_step = (self.cart_vel_bound[1]- self.cart_vel_bound[0])/self.states_per_dimention\n",
        "#         self.pole_angle_step = (self.pole_angle_bound[1] - self.pole_angle_bound[0])/self.states_per_dimention\n",
        "#         self.pole_angleVel_step = (self.pole_angleVel_bound[1]- self.pole_angleVel_bound[0])/self.states_per_dimention\n",
        "        \n",
        "\n",
        "#         ### END SOLUTION\n",
        "        \n",
        "#     def observation(self, obs):\n",
        "#         ### BEGIN SOLUTION\n",
        "#         # Quantize the observations (states)\n",
        "#         new_obs = [0,0,0,0]\n",
        "#         new_obs[0] = int((obs[0]-self.cart_pos_bound[0])/self.cart_pos_step)\n",
        "#         new_obs[1] = int((obs[1]-self.cart_vel_bound[0])/self.cart_vel_step)\n",
        "#         new_obs[2] = int((obs[2]-self.pole_angle_bound[0])/self.pole_angle_step)\n",
        "#         new_obs[3] = int((obs[3]-self.pole_angleVel_bound[0])/self.pole_angleVel_step)\n",
        "\n",
        "#         # Once quantized each dimension compute a single observation index\n",
        "#         return int(new_obs[0]*self.states_per_dimention**3+new_obs[1]*self.states_per_dimention**2+new_obs[2]*self.states_per_dimention**1 + new_obs[3]*self.states_per_dimention**0)\n",
        "\n",
        "#         ### END SOLUTION\n",
        "#         # return new_obs\n",
        "\n",
        "# # Initialize environment, wrap to render\n",
        "# env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# # Wrap to discretize the observation space\n",
        "# env = ObservationWrapper(env)\n",
        "\n",
        "# ### BEGIN SOLUTION\n",
        "# # Initialize policy\n",
        "\n",
        "# observation, info = env.reset()\n",
        "\n",
        "# while True:\n",
        "#     env.render()\n",
        "    \n",
        "#     action = env.action_space.sample() \n",
        "#     # print(action)\n",
        "#     state_next, reward, terminated, truncated, info = env.step(action)\n",
        "#     print(state_next)\n",
        "#     done = terminated or truncated\n",
        "#     # i+=1\n",
        "#     # if i==50:\n",
        "#     #     break\n",
        "#     if done: \n",
        "#       break\n",
        "            \n",
        "# env.close()\n"
      ],
      "metadata": {
        "id": "3ILgJ_gJm89C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ObservationWrapper(gym.ObservationWrapper):\n",
        "    '''\n",
        "    n_states(int): number of state of the env.\n",
        "                It has to be the some power of N = env.observation_space.shape[0]\n",
        "    bounds(list of tuples): bounds of the states of the env.(taken from the gymnasium documentation).\n",
        "                each tuple gives the bounds of the states\n",
        "    '''\n",
        "    def __init__(self, env, n_states=4096, bounds=[(-4.8,4.8),(-50,50),( -0.418, 0.418),(-20,20)]):\n",
        "        super().__init__(env)\n",
        "\n",
        "        self.n_states = n_states\n",
        "        self.bounds = bounds\n",
        "        assert isinstance(env.observation_space, gym.spaces.box.Box)\n",
        "\n",
        "        # Get the dimensions of the observation space\n",
        "        self.dimension = env.observation_space.shape[0] \n",
        "        self.states_per_dimention = int(n_states**(1/self.dimension)) #we give equal number of state per dimension\n",
        "        \n",
        "        # calculating steps for each state and storing in a list\n",
        "        self.step_list = []\n",
        "        for state_bound in bounds:\n",
        "            step = (state_bound[1] - state_bound[0])/self.states_per_dimention\n",
        "            self.step_list.append(step)\n",
        "\n",
        "        \n",
        "    def get_quant_observation(self, new_obs):\n",
        "        ##calculate the new quantized state index from a list of states.  (like converting number systems)\n",
        "        # print(new_obs)\n",
        "        quantized_obs = 0\n",
        "        dimension = self.dimension\n",
        "        for i, obs in enumerate(new_obs):\n",
        "            quantized_obs+= obs*self.states_per_dimention**(dimension-i-1)\n",
        "        \n",
        "        # print(quantized_obs)\n",
        "        return int(quantized_obs)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        ### BEGIN SOLUTION\n",
        "        # Quantize the observations (states)\n",
        "        new_obs = []\n",
        "        # print(obs)\n",
        "        for i,observation_state in enumerate(obs):\n",
        "            new_obs.append(int((observation_state-self.bounds[i][0])/self.step_list[i]))\n",
        "            \n",
        "        # Once quantized each dimension compute a single observation index\n",
        "        return self.get_quant_observation(new_obs)\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "BQBq5DAK5LKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_policy(env, gamma, policy, n_episodes):\n",
        "    episodes_to_goal = 0\n",
        "    steps_to_goal = []\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    for episode in range(n_episodes):\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        episodes_to_goal+= len(states)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return episodes_to_goal/n_episodes,steps_to_goal"
      ],
      "metadata": {
        "id": "MF8aORgvUNbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment, wrap to render\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Wrap to discretize the observation space\n",
        "env = ObservationWrapper(env)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Initialize policy\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "q_initial = np.zeros((4096, 2))\n",
        "qlearn_policy = EpsilonGreedyPolicy(env.action_space, q_initial,\n",
        "                                    epsilon=1, epsilon_decay=0.999, epsilon_min=0.01)\n",
        "\n",
        "gamma = 0.9\n",
        "n_episodes = 20000\n",
        "alpha = 0.1\n",
        "n_episodes_score = 100\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Train the policy\n",
        "qlearn_policy.train()\n",
        "policy_learn_qlearn(env, qlearn_policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500)\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Evaluate the policy\n",
        "qlearn_policy.eval()\n",
        "avg_episodes_to_goal = score_policy(env, gamma, qlearn_policy, n_episodes_score)\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(f'Policy Q-learn: {avg_episodes_to_goal}')\n",
        "\n"
      ],
      "metadata": {
        "id": "2pKCQM0sMJwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qlearn_policy.eval()\n",
        "avg_episodes_to_goal = score_policy(env, gamma, qlearn_policy, n_episodes_score)\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(f'Policy Q-learn: {avg_episodes_to_goal}')\n"
      ],
      "metadata": {
        "id": "YJprH1qGvCtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Wrap to discretize the observation space\n",
        "# env = ObservationWrapper(env)\n",
        "\n",
        "env = RecordVideo(ObservationWrapper(env))\n",
        "qlearn_policy.eval()\n",
        "for i in range(1):\n",
        "    state_next, reward, terminated, truncated = sample_episode(env, qlearn_policy)\n",
        "    \n",
        "env.close()"
      ],
      "metadata": {
        "id": "XUzS2Xb3SMpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RecordVideo(gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\"))\n",
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "\n",
        "observation, info = env.reset()\n",
        "i=0\n",
        "while True:\n",
        "    env.render()\n",
        "    \n",
        "    action = env.action_space.sample() \n",
        "    state_next, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    \n",
        "    if done: \n",
        "      break\n",
        "            \n",
        "env.close()"
      ],
      "metadata": {
        "id": "7MhFhVPaRtGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_policy(env, gamma, policy, n_episodes):\n",
        "    episodes_to_goal = 0\n",
        "    steps_to_goal = []\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    for episode in range(n_episodes):\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        if rewards[-1]==0:\n",
        "            episodes_to_goal+=1\n",
        "            steps_to_goal.append(len(rewards))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return episodes_to_goal/n_episodes, np.mean(steps_to_goal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n"
      ],
      "metadata": {
        "id": "ZfaIvJysVopj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import BoundArguments\n",
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Wrap to discretize the observation space\n",
        "n_states = env.observation_space.shape[0]**5\n",
        "action_space = env.action_space.n\n",
        "bounds = [(-1,1),(-1,1),(-1,1),(-1,1),(-12.567,12.567),(-28.274, 28.274 )]\n",
        "env = ObservationWrapper(env, n_states = n_states, bounds = bounds)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Initialize policy\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "q_initial = np.zeros((n_states, action_space))\n",
        "qlearn_policy = EpsilonGreedyPolicy(env.action_space, q_initial,\n",
        "                                    epsilon=1, epsilon_decay=0.999, epsilon_min=0.001)\n",
        "\n",
        "gamma = 0.9\n",
        "n_episodes = 5000\n",
        "alpha = 0.2\n",
        "n_episodes_score = 100\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Train the policy\n",
        "qlearn_policy.train()\n",
        "policy_learn_qlearn(env, qlearn_policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500)\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Evaluate the policy\n",
        "qlearn_policy.eval()\n",
        "avg_episodes_to_goal, mean_steps = score_policy(env, gamma, qlearn_policy, n_episodes_score)\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(f'Policy Q-learn: {avg_episodes_to_goal:.3%}, avg steps: {mean_steps}')\n",
        "\n"
      ],
      "metadata": {
        "id": "3TWh4XK3Ut4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Wrap to discretize the observation space\n",
        "n_states = env.observation_space.shape[0]**5\n",
        "action_space = env.action_space.n\n",
        "bounds = [(-1,1),(-1,1),(-1,1),(-1,1),(-12.567,12.567),(-28.274, 28.274 )]\n",
        "env = ObservationWrapper(env, n_states = n_states, bounds = bounds)\n",
        "env = RecordVideo(env)\n",
        "\n",
        "qlearn_policy.eval()\n",
        "for i in range(2):\n",
        "    state_next, reward, terminated, truncated = sample_episode(env, qlearn_policy)\n",
        "    \n",
        "env.close()"
      ],
      "metadata": {
        "id": "BPt7mhe3bEv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "5dJQS5HNOb2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RecordVideo(gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = False,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = False,\n",
        "    #wind_power = 15.0,\n",
        "    #turbulence_power = 1.5,\n",
        "    render_mode=\"rgb_array\"\n",
        "))\n",
        "observation, info = env.reset()\n",
        "i=0\n",
        "while True:\n",
        "    env.render()\n",
        "    \n",
        "    action = env.action_space.sample() \n",
        "    state_next, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    # print(reward)\n",
        "    if done: \n",
        "      break\n",
        "            \n",
        "env.close()"
      ],
      "metadata": {
        "id": "OW3InND9OLHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_policy(env, gamma, policy, n_episodes):\n",
        "    episodes_to_goal = []\n",
        "    steps_to_goal = []\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    for episode in range(n_episodes):\n",
        "        states, actions, rewards, dones = sample_episode(env, policy, reset = True)\n",
        "        # if rewards[-1]==100:\n",
        "        #     episodes_to_goal+=1\n",
        "        #     steps_to_goal.append(len(rewards))\n",
        "        episodes_to_goal.append(rewards[-1])\n",
        "        steps_to_goal.append(len(actions))\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return np.mean(episodes_to_goal), np.mean(steps_to_goal)\n",
        "\n",
        "env = gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = False,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = False,\n",
        "    #wind_power = 15.0,\n",
        "    #turbulence_power = 1.5,\n",
        "    render_mode=\"rgb_array\"\n",
        ")\n",
        "\n",
        "# Wrap to discretize the observation space\n",
        "n_states = env.observation_space.shape[0]**6\n",
        "action_space = env.action_space.n\n",
        "\n",
        "bounds = [(-1.5,1.5),(-1.5,1.5), (-5,5),(-5,5),(-3.14,3.14),(-5, 5 ),(-0,1),(-0,1)]\n",
        "env = ObservationWrapper(env, n_states = n_states, bounds = bounds)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Initialize policy\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "q_initial = np.zeros((n_states, action_space))\n",
        "qlearn_policy = EpsilonGreedyPolicy(env.action_space, q_initial,\n",
        "                                    epsilon=1, epsilon_decay=0.9999999, epsilon_min=0.01)\n",
        "\n",
        "gamma = 0.9\n",
        "n_episodes = 500000\n",
        "alpha = 0.2\n",
        "n_episodes_score = 100\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Train the policy\n",
        "qlearn_policy.train()\n",
        "policy_learn_qlearn(env, qlearn_policy, gamma, n_episodes, alpha, max_n_steps=1000, print_every=500)\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Evaluate the policy\n",
        "qlearn_policy.eval()\n",
        "avg_episodes_to_goal, mean_steps = score_policy(env, gamma, qlearn_policy, n_episodes_score)\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(f'Policy Q-learn: {avg_episodes_to_goal}, avg steps: {mean_steps}')\n",
        "\n",
        "\n",
        "\n",
        "env = gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = False,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = False,\n",
        "    #wind_power = 15.0,\n",
        "    #turbulence_power = 1.5,\n",
        "    render_mode=\"rgb_array\"\n",
        ")\n",
        "\n",
        "# Wrap to discretize the observation space\n",
        "n_states = env.observation_space.shape[0]**6\n",
        "action_space = env.action_space.n\n",
        "\n",
        "bounds = [(-1.5,1.5),(-1.5,1.5), (-5,5),(-5,5),(-3.14,3.14),(-5, 5 ),(-0,1),(-0,1)]\n",
        "env = ObservationWrapper(env, n_states = n_states, bounds = bounds)\n",
        "env = RecordVideo(env)\n",
        "\n",
        "qlearn_policy.eval()\n",
        "for i in range(2):\n",
        "    state_next, reward, terminated, truncated = sample_episode(env, qlearn_policy)\n",
        "    \n",
        "env.close()"
      ],
      "metadata": {
        "id": "TJSpnL--PvmM",
        "outputId": "56e499da-5d8f-49ac-9dd3-f6a1f0d1cf03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep: 0, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"69.74\n",
            "\n",
            "ep: 500, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"197.26\n",
            "\n",
            "ep: 1000, epsilon: 1.000\n",
            "Policy Q-learn: -99.00114372062893, mean\"188.78\n",
            "\n",
            "ep: 1500, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"142.22\n",
            "\n",
            "ep: 2000, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"190.13\n",
            "\n",
            "ep: 2500, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"219.95\n",
            "\n",
            "ep: 3000, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"161.39\n",
            "\n",
            "ep: 3500, epsilon: 1.000\n",
            "Policy Q-learn: -99.0225654842259, mean\"343.7\n",
            "\n",
            "ep: 4000, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"309.67\n",
            "\n",
            "ep: 4500, epsilon: 1.000\n",
            "Policy Q-learn: -100.0, mean\"155.57\n",
            "\n",
            "ep: 5000, epsilon: 1.000\n",
            "Policy Q-learn: -96.99569330648364, mean\"158.17\n",
            "\n",
            "ep: 5500, epsilon: 0.999\n",
            "Policy Q-learn: -96.00129699156678, mean\"158.3\n",
            "\n",
            "ep: 6000, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"308.13\n",
            "\n",
            "ep: 6500, epsilon: 0.999\n",
            "Policy Q-learn: -98.00410827575901, mean\"227.07\n",
            "\n",
            "ep: 7000, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"312.55\n",
            "\n",
            "ep: 7500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"195.71\n",
            "\n",
            "ep: 8000, epsilon: 0.999\n",
            "Policy Q-learn: -93.96838468176949, mean\"285.45\n",
            "\n",
            "ep: 8500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"200.65\n",
            "\n",
            "ep: 9000, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"148.36\n",
            "\n",
            "ep: 9500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"171.94\n",
            "\n",
            "ep: 10000, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"210.01\n",
            "\n",
            "ep: 10500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"226.26\n",
            "\n",
            "ep: 11000, epsilon: 0.999\n",
            "Policy Q-learn: -98.99419138875042, mean\"161.45\n",
            "\n",
            "ep: 11500, epsilon: 0.999\n",
            "Policy Q-learn: -98.00565035976783, mean\"166.37\n",
            "\n",
            "ep: 12000, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"185.23\n",
            "\n",
            "ep: 12500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"159.1\n",
            "\n",
            "ep: 13000, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"201.44\n",
            "\n",
            "ep: 13500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"235.19\n",
            "\n",
            "ep: 14000, epsilon: 0.999\n",
            "Policy Q-learn: -99.01542734445465, mean\"303.11\n",
            "\n",
            "ep: 14500, epsilon: 0.999\n",
            "Policy Q-learn: -100.0, mean\"170.89\n",
            "\n",
            "ep: 15000, epsilon: 0.999\n",
            "Policy Q-learn: -99.03511681175728, mean\"300.17\n",
            "\n",
            "ep: 15500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"147.85\n",
            "\n",
            "ep: 16000, epsilon: 0.998\n",
            "Policy Q-learn: -98.9990393344544, mean\"157.44\n",
            "\n",
            "ep: 16500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"159.66\n",
            "\n",
            "ep: 17000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"232.36\n",
            "\n",
            "ep: 17500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"139.85\n",
            "\n",
            "ep: 18000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"171.46\n",
            "\n",
            "ep: 18500, epsilon: 0.998\n",
            "Policy Q-learn: -98.0, mean\"141.36\n",
            "\n",
            "ep: 19000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"147.74\n",
            "\n",
            "ep: 19500, epsilon: 0.998\n",
            "Policy Q-learn: -98.98857262663739, mean\"305.67\n",
            "\n",
            "ep: 20000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"160.98\n",
            "\n",
            "ep: 20500, epsilon: 0.998\n",
            "Policy Q-learn: -97.99482305461845, mean\"331.01\n",
            "\n",
            "ep: 21000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"299.71\n",
            "\n",
            "ep: 21500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"145.18\n",
            "\n",
            "ep: 22000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"140.29\n",
            "\n",
            "ep: 22500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"133.76\n",
            "\n",
            "ep: 23000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"162.59\n",
            "\n",
            "ep: 23500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"117.55\n",
            "\n",
            "ep: 24000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"231.36\n",
            "\n",
            "ep: 24500, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"186.47\n",
            "\n",
            "ep: 25000, epsilon: 0.998\n",
            "Policy Q-learn: -100.0, mean\"159.57\n",
            "\n",
            "ep: 25500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"301.06\n",
            "\n",
            "ep: 26000, epsilon: 0.997\n",
            "Policy Q-learn: -97.9975833667361, mean\"256.77\n",
            "\n",
            "ep: 26500, epsilon: 0.997\n",
            "Policy Q-learn: -82.07512008093838, mean\"480.27\n",
            "\n",
            "ep: 27000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"148.5\n",
            "\n",
            "ep: 27500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"136.98\n",
            "\n",
            "ep: 28000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"142.52\n",
            "\n",
            "ep: 28500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"164.28\n",
            "\n",
            "ep: 29000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"133.43\n",
            "\n",
            "ep: 29500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"214.03\n",
            "\n",
            "ep: 30000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"160.45\n",
            "\n",
            "ep: 30500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"264.5\n",
            "\n",
            "ep: 31000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"217.08\n",
            "\n",
            "ep: 31500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"140.84\n",
            "\n",
            "ep: 32000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"227.95\n",
            "\n",
            "ep: 32500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"143.08\n",
            "\n",
            "ep: 33000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"210.09\n",
            "\n",
            "ep: 33500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"156.54\n",
            "\n",
            "ep: 34000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"319.34\n",
            "\n",
            "ep: 34500, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"156.56\n",
            "\n",
            "ep: 35000, epsilon: 0.997\n",
            "Policy Q-learn: -100.0, mean\"196.8\n",
            "\n",
            "ep: 35500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"215.33\n",
            "\n",
            "ep: 36000, epsilon: 0.996\n",
            "Policy Q-learn: -99.02051629790908, mean\"228.76\n",
            "\n",
            "ep: 36500, epsilon: 0.996\n",
            "Policy Q-learn: -99.01061081442032, mean\"220.59\n",
            "\n",
            "ep: 37000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"235.22\n",
            "\n",
            "ep: 37500, epsilon: 0.996\n",
            "Policy Q-learn: -99.00179381685109, mean\"291.88\n",
            "\n",
            "ep: 38000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"156.47\n",
            "\n",
            "ep: 38500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"186.3\n",
            "\n",
            "ep: 39000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"274.47\n",
            "\n",
            "ep: 39500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"155.94\n",
            "\n",
            "ep: 40000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"159.49\n",
            "\n",
            "ep: 40500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"250.26\n",
            "\n",
            "ep: 41000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"207.29\n",
            "\n",
            "ep: 41500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"159.56\n",
            "\n",
            "ep: 42000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"242.77\n",
            "\n",
            "ep: 42500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"317.62\n",
            "\n",
            "ep: 43000, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"258.48\n",
            "\n",
            "ep: 43500, epsilon: 0.996\n",
            "Policy Q-learn: -100.0, mean\"169.19\n",
            "\n",
            "ep: 44000, epsilon: 0.996\n",
            "Policy Q-learn: -83.1615107755428, mean\"362.24\n",
            "\n",
            "ep: 44500, epsilon: 0.996\n",
            "Policy Q-learn: -94.95047661172332, mean\"332.51\n",
            "\n",
            "ep: 45000, epsilon: 0.996\n",
            "Policy Q-learn: -65.07693211881215, mean\"577.71\n",
            "\n",
            "ep: 45500, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"179.82\n",
            "\n",
            "ep: 46000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"256.42\n",
            "\n",
            "ep: 46500, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"202.02\n",
            "\n",
            "ep: 47000, epsilon: 0.995\n",
            "Policy Q-learn: -98.0, mean\"215.39\n",
            "\n",
            "ep: 47500, epsilon: 0.995\n",
            "Policy Q-learn: -96.03623508193589, mean\"304.46\n",
            "\n",
            "ep: 48000, epsilon: 0.995\n",
            "Policy Q-learn: -93.00834633367504, mean\"372.4\n",
            "\n",
            "ep: 48500, epsilon: 0.995\n",
            "Policy Q-learn: -56.077471024033436, mean\"632.73\n",
            "\n",
            "ep: 49000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"224.72\n",
            "\n",
            "ep: 49500, epsilon: 0.995\n",
            "Policy Q-learn: -99.00463043397971, mean\"194.4\n",
            "\n",
            "ep: 50000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"161.59\n",
            "\n",
            "ep: 50500, epsilon: 0.995\n",
            "Policy Q-learn: -98.9942274998996, mean\"267.87\n",
            "\n",
            "ep: 51000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"136.54\n",
            "\n",
            "ep: 51500, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"154.91\n",
            "\n",
            "ep: 52000, epsilon: 0.995\n",
            "Policy Q-learn: -98.99677580582689, mean\"287.89\n",
            "\n",
            "ep: 52500, epsilon: 0.995\n",
            "Policy Q-learn: -90.04328469218301, mean\"373.86\n",
            "\n",
            "ep: 53000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"149.62\n",
            "\n",
            "ep: 53500, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"160.33\n",
            "\n",
            "ep: 54000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"196.86\n",
            "\n",
            "ep: 54500, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"177.53\n",
            "\n",
            "ep: 55000, epsilon: 0.995\n",
            "Policy Q-learn: -100.0, mean\"139.35\n",
            "\n",
            "ep: 55500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"164.48\n",
            "\n",
            "ep: 56000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"139.04\n",
            "\n",
            "ep: 56500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"169.24\n",
            "\n",
            "ep: 57000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"192.3\n",
            "\n",
            "ep: 57500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"283.42\n",
            "\n",
            "ep: 58000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"196.94\n",
            "\n",
            "ep: 58500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"155.84\n",
            "\n",
            "ep: 59000, epsilon: 0.994\n",
            "Policy Q-learn: -94.15402541246695, mean\"342.38\n",
            "\n",
            "ep: 59500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"219.4\n",
            "\n",
            "ep: 60000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"145.65\n",
            "\n",
            "ep: 60500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"160.94\n",
            "\n",
            "ep: 61000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"227.74\n",
            "\n",
            "ep: 61500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"257.35\n",
            "\n",
            "ep: 62000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"201.66\n",
            "\n",
            "ep: 62500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"171.17\n",
            "\n",
            "ep: 63000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"222.82\n",
            "\n",
            "ep: 63500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"226.64\n",
            "\n",
            "ep: 64000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"273.95\n",
            "\n",
            "ep: 64500, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"177.05\n",
            "\n",
            "ep: 65000, epsilon: 0.994\n",
            "Policy Q-learn: -100.0, mean\"156.43\n",
            "\n",
            "ep: 65500, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"144.08\n",
            "\n",
            "ep: 66000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"159.99\n",
            "\n",
            "ep: 66500, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"149.67\n",
            "\n",
            "ep: 67000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"184.46\n",
            "\n",
            "ep: 67500, epsilon: 0.993\n",
            "Policy Q-learn: -92.08063212395513, mean\"489.46\n",
            "\n",
            "ep: 68000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"213.08\n",
            "\n",
            "ep: 68500, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"153.74\n",
            "\n",
            "ep: 69000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"246.36\n",
            "\n",
            "ep: 69500, epsilon: 0.993\n",
            "Policy Q-learn: -89.99121877099302, mean\"300.82\n",
            "\n",
            "ep: 70000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"227.8\n",
            "\n",
            "ep: 70500, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"197.1\n",
            "\n",
            "ep: 71000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"181.19\n",
            "\n",
            "ep: 71500, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"158.3\n",
            "\n",
            "ep: 72000, epsilon: 0.993\n",
            "Policy Q-learn: -93.9857290638119, mean\"302.83\n",
            "\n",
            "ep: 72500, epsilon: 0.993\n",
            "Policy Q-learn: -97.9953629148605, mean\"273.93\n",
            "\n",
            "ep: 73000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"240.4\n",
            "\n",
            "ep: 73500, epsilon: 0.993\n",
            "Policy Q-learn: -92.07174942921893, mean\"338.01\n",
            "\n",
            "ep: 74000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"143.82\n",
            "\n",
            "ep: 74500, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"157.3\n",
            "\n",
            "ep: 75000, epsilon: 0.993\n",
            "Policy Q-learn: -100.0, mean\"188.84\n",
            "\n",
            "ep: 75500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"252.32\n",
            "\n",
            "ep: 76000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"256.02\n",
            "\n",
            "ep: 76500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"330.43\n",
            "\n",
            "ep: 77000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"154.73\n",
            "\n",
            "ep: 77500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"166.12\n",
            "\n",
            "ep: 78000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"246.81\n",
            "\n",
            "ep: 78500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"286.6\n",
            "\n",
            "ep: 79000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"140.91\n",
            "\n",
            "ep: 79500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"181.29\n",
            "\n",
            "ep: 80000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"175.84\n",
            "\n",
            "ep: 80500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"222.43\n",
            "\n",
            "ep: 81000, epsilon: 0.992\n",
            "Policy Q-learn: -94.98235733815513, mean\"257.94\n",
            "\n",
            "ep: 81500, epsilon: 0.992\n",
            "Policy Q-learn: -90.02274382680228, mean\"338.6\n",
            "\n",
            "ep: 82000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"183.04\n",
            "\n",
            "ep: 82500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"162.3\n",
            "\n",
            "ep: 83000, epsilon: 0.992\n",
            "Policy Q-learn: -98.0, mean\"237.08\n",
            "\n",
            "ep: 83500, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"155.82\n",
            "\n",
            "ep: 84000, epsilon: 0.992\n",
            "Policy Q-learn: -100.0, mean\"178.92\n",
            "\n",
            "ep: 84500, epsilon: 0.992\n",
            "Policy Q-learn: -86.09334339159119, mean\"465.2\n",
            "\n",
            "ep: 85000, epsilon: 0.992\n",
            "Policy Q-learn: -83.05111969140864, mean\"438.3\n",
            "\n",
            "ep: 85500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"183.0\n",
            "\n",
            "ep: 86000, epsilon: 0.991\n",
            "Policy Q-learn: -94.21308941085162, mean\"319.15\n",
            "\n",
            "ep: 86500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"200.63\n",
            "\n",
            "ep: 87000, epsilon: 0.991\n",
            "Policy Q-learn: -97.90596379558949, mean\"181.04\n",
            "\n",
            "ep: 87500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"138.84\n",
            "\n",
            "ep: 88000, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"235.39\n",
            "\n",
            "ep: 88500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"265.69\n",
            "\n",
            "ep: 89000, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"301.96\n",
            "\n",
            "ep: 89500, epsilon: 0.991\n",
            "Policy Q-learn: -98.00221931440984, mean\"195.96\n",
            "\n",
            "ep: 90000, epsilon: 0.991\n",
            "Policy Q-learn: -96.9945649256103, mean\"238.55\n",
            "\n",
            "ep: 90500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"124.95\n",
            "\n",
            "ep: 91000, epsilon: 0.991\n",
            "Policy Q-learn: -98.99688669416116, mean\"162.61\n",
            "\n",
            "ep: 91500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"145.14\n",
            "\n",
            "ep: 92000, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"204.53\n",
            "\n",
            "ep: 92500, epsilon: 0.991\n",
            "Policy Q-learn: -95.04126779220013, mean\"356.69\n",
            "\n",
            "ep: 93000, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"200.39\n",
            "\n",
            "ep: 93500, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"242.57\n",
            "\n",
            "ep: 94000, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"176.71\n",
            "\n",
            "ep: 94500, epsilon: 0.991\n",
            "Policy Q-learn: -90.00027025757765, mean\"236.82\n",
            "\n",
            "ep: 95000, epsilon: 0.991\n",
            "Policy Q-learn: -100.0, mean\"166.45\n",
            "\n",
            "ep: 95500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"160.77\n",
            "\n",
            "ep: 96000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"267.28\n",
            "\n",
            "ep: 96500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"149.28\n",
            "\n",
            "ep: 97000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"148.26\n",
            "\n",
            "ep: 97500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"177.7\n",
            "\n",
            "ep: 98000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"150.9\n",
            "\n",
            "ep: 98500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"129.15\n",
            "\n",
            "ep: 99000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"251.78\n",
            "\n",
            "ep: 99500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"158.73\n",
            "\n",
            "ep: 100000, epsilon: 0.990\n",
            "Policy Q-learn: -85.13364619675555, mean\"392.15\n",
            "\n",
            "ep: 100500, epsilon: 0.990\n",
            "Policy Q-learn: -96.10940682709938, mean\"376.81\n",
            "\n",
            "ep: 101000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"145.4\n",
            "\n",
            "ep: 101500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"230.44\n",
            "\n",
            "ep: 102000, epsilon: 0.990\n",
            "Policy Q-learn: -66.10816290569731, mean\"524.13\n",
            "\n",
            "ep: 102500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"154.32\n",
            "\n",
            "ep: 103000, epsilon: 0.990\n",
            "Policy Q-learn: -93.98139861486544, mean\"282.09\n",
            "\n",
            "ep: 103500, epsilon: 0.990\n",
            "Policy Q-learn: -95.99683707692424, mean\"169.31\n",
            "\n",
            "ep: 104000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"188.31\n",
            "\n",
            "ep: 104500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"132.98\n",
            "\n",
            "ep: 105000, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"332.06\n",
            "\n",
            "ep: 105500, epsilon: 0.990\n",
            "Policy Q-learn: -100.0, mean\"185.33\n",
            "\n",
            "ep: 106000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"135.14\n",
            "\n",
            "ep: 106500, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"124.14\n",
            "\n",
            "ep: 107000, epsilon: 0.989\n",
            "Policy Q-learn: -98.99893964686004, mean\"152.14\n",
            "\n",
            "ep: 107500, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"153.52\n",
            "\n",
            "ep: 108000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"157.85\n",
            "\n",
            "ep: 108500, epsilon: 0.989\n",
            "Policy Q-learn: -98.07156564238674, mean\"292.51\n",
            "\n",
            "ep: 109000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"150.61\n",
            "\n",
            "ep: 109500, epsilon: 0.989\n",
            "Policy Q-learn: -87.26592953328313, mean\"467.2\n",
            "\n",
            "ep: 110000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"247.6\n",
            "\n",
            "ep: 110500, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"154.4\n",
            "\n",
            "ep: 111000, epsilon: 0.989\n",
            "Policy Q-learn: -99.03348737329092, mean\"305.45\n",
            "\n",
            "ep: 111500, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"269.64\n",
            "\n",
            "ep: 112000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"138.9\n",
            "\n",
            "ep: 112500, epsilon: 0.989\n",
            "Policy Q-learn: -79.0094661496931, mean\"531.9\n",
            "\n",
            "ep: 113000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"164.01\n",
            "\n",
            "ep: 113500, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"142.02\n",
            "\n",
            "ep: 114000, epsilon: 0.989\n",
            "Policy Q-learn: -99.02567612382187, mean\"402.2\n",
            "\n",
            "ep: 114500, epsilon: 0.989\n",
            "Policy Q-learn: -87.29395977849258, mean\"370.82\n",
            "\n",
            "ep: 115000, epsilon: 0.989\n",
            "Policy Q-learn: -100.0, mean\"164.62\n",
            "\n",
            "ep: 115500, epsilon: 0.989\n",
            "Policy Q-learn: -96.94608324672399, mean\"236.25\n",
            "\n",
            "ep: 116000, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"124.03\n",
            "\n",
            "ep: 116500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"159.85\n",
            "\n",
            "ep: 117000, epsilon: 0.988\n",
            "Policy Q-learn: -99.03600966485334, mean\"158.38\n",
            "\n",
            "ep: 117500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"211.66\n",
            "\n",
            "ep: 118000, epsilon: 0.988\n",
            "Policy Q-learn: -95.01412080136225, mean\"330.27\n",
            "\n",
            "ep: 118500, epsilon: 0.988\n",
            "Policy Q-learn: -95.9851219850351, mean\"422.24\n",
            "\n",
            "ep: 119000, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"158.48\n",
            "\n",
            "ep: 119500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"162.14\n",
            "\n",
            "ep: 120000, epsilon: 0.988\n",
            "Policy Q-learn: -88.1093233162474, mean\"443.83\n",
            "\n",
            "ep: 120500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"158.34\n",
            "\n",
            "ep: 121000, epsilon: 0.988\n",
            "Policy Q-learn: -85.02131939422249, mean\"384.57\n",
            "\n",
            "ep: 121500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"240.5\n",
            "\n",
            "ep: 122000, epsilon: 0.988\n",
            "Policy Q-learn: -98.06081884188758, mean\"283.35\n",
            "\n",
            "ep: 122500, epsilon: 0.988\n",
            "Policy Q-learn: -86.0385271102543, mean\"235.84\n",
            "\n",
            "ep: 123000, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"125.44\n",
            "\n",
            "ep: 123500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"225.25\n",
            "\n",
            "ep: 124000, epsilon: 0.988\n",
            "Policy Q-learn: -59.982234194659505, mean\"638.82\n",
            "\n",
            "ep: 124500, epsilon: 0.988\n",
            "Policy Q-learn: -99.00226118244592, mean\"204.67\n",
            "\n",
            "ep: 125000, epsilon: 0.988\n",
            "Policy Q-learn: -97.9638532413359, mean\"414.76\n",
            "\n",
            "ep: 125500, epsilon: 0.988\n",
            "Policy Q-learn: -100.0, mean\"156.64\n",
            "\n",
            "ep: 126000, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"141.84\n",
            "\n",
            "ep: 126500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"221.48\n",
            "\n",
            "ep: 127000, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"243.43\n",
            "\n",
            "ep: 127500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"136.27\n",
            "\n",
            "ep: 128000, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"150.67\n",
            "\n",
            "ep: 128500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"132.77\n",
            "\n",
            "ep: 129000, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"161.99\n",
            "\n",
            "ep: 129500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"193.46\n",
            "\n",
            "ep: 130000, epsilon: 0.987\n",
            "Policy Q-learn: -91.01206985057551, mean\"354.15\n",
            "\n",
            "ep: 130500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"220.23\n",
            "\n",
            "ep: 131000, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"152.39\n",
            "\n",
            "ep: 131500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"191.45\n",
            "\n",
            "ep: 132000, epsilon: 0.987\n",
            "Policy Q-learn: -94.06475653530666, mean\"363.95\n",
            "\n",
            "ep: 132500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"145.77\n",
            "\n",
            "ep: 133000, epsilon: 0.987\n",
            "Policy Q-learn: -98.99983777369327, mean\"149.65\n",
            "\n",
            "ep: 133500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"259.26\n",
            "\n",
            "ep: 134000, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"145.34\n",
            "\n",
            "ep: 134500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"137.65\n",
            "\n",
            "ep: 135000, epsilon: 0.987\n",
            "Policy Q-learn: -91.9646198022099, mean\"404.04\n",
            "\n",
            "ep: 135500, epsilon: 0.987\n",
            "Policy Q-learn: -100.0, mean\"152.77\n",
            "\n",
            "ep: 136000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"170.86\n",
            "\n",
            "ep: 136500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"154.95\n",
            "\n",
            "ep: 137000, epsilon: 0.986\n",
            "Policy Q-learn: -99.01261310416118, mean\"214.41\n",
            "\n",
            "ep: 137500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"161.3\n",
            "\n",
            "ep: 138000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"207.07\n",
            "\n",
            "ep: 138500, epsilon: 0.986\n",
            "Policy Q-learn: -98.00716215289302, mean\"243.96\n",
            "\n",
            "ep: 139000, epsilon: 0.986\n",
            "Policy Q-learn: -93.02944185966253, mean\"400.0\n",
            "\n",
            "ep: 139500, epsilon: 0.986\n",
            "Policy Q-learn: -98.00113263856593, mean\"310.69\n",
            "\n",
            "ep: 140000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"142.64\n",
            "\n",
            "ep: 140500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"149.37\n",
            "\n",
            "ep: 141000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"245.09\n",
            "\n",
            "ep: 141500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"160.39\n",
            "\n",
            "ep: 142000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"143.23\n",
            "\n",
            "ep: 142500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"277.13\n",
            "\n",
            "ep: 143000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"149.61\n",
            "\n",
            "ep: 143500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"146.45\n",
            "\n",
            "ep: 144000, epsilon: 0.986\n",
            "Policy Q-learn: -75.02071718571212, mean\"444.66\n",
            "\n",
            "ep: 144500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"185.95\n",
            "\n",
            "ep: 145000, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"148.03\n",
            "\n",
            "ep: 145500, epsilon: 0.986\n",
            "Policy Q-learn: -100.0, mean\"203.39\n",
            "\n",
            "ep: 146000, epsilon: 0.986\n",
            "Policy Q-learn: -98.99538636234789, mean\"265.45\n",
            "\n",
            "ep: 146500, epsilon: 0.985\n",
            "Policy Q-learn: -92.04776535441484, mean\"311.91\n",
            "\n",
            "ep: 147000, epsilon: 0.985\n",
            "Policy Q-learn: -96.97147576922607, mean\"282.57\n",
            "\n",
            "ep: 147500, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"201.3\n",
            "\n",
            "ep: 148000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"147.79\n",
            "\n",
            "ep: 148500, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"157.6\n",
            "\n",
            "ep: 149000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"196.85\n",
            "\n",
            "ep: 149500, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"224.09\n",
            "\n",
            "ep: 150000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"145.22\n",
            "\n",
            "ep: 150500, epsilon: 0.985\n",
            "Policy Q-learn: -95.97801269352126, mean\"319.58\n",
            "\n",
            "ep: 151000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"162.02\n",
            "\n",
            "ep: 151500, epsilon: 0.985\n",
            "Policy Q-learn: -97.0034566336581, mean\"216.17\n",
            "\n",
            "ep: 152000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"185.64\n",
            "\n",
            "ep: 152500, epsilon: 0.985\n",
            "Policy Q-learn: -97.99508888676813, mean\"251.4\n",
            "\n",
            "ep: 153000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"166.93\n",
            "\n",
            "ep: 153500, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"161.67\n",
            "\n",
            "ep: 154000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"147.01\n",
            "\n",
            "ep: 154500, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"210.34\n",
            "\n",
            "ep: 155000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"151.95\n",
            "\n",
            "ep: 155500, epsilon: 0.985\n",
            "Policy Q-learn: -99.02031077869796, mean\"286.44\n",
            "\n",
            "ep: 156000, epsilon: 0.985\n",
            "Policy Q-learn: -100.0, mean\"194.38\n",
            "\n",
            "ep: 156500, epsilon: 0.984\n",
            "Policy Q-learn: -98.98077399632224, mean\"282.19\n",
            "\n",
            "ep: 157000, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"207.17\n",
            "\n",
            "ep: 157500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"229.21\n",
            "\n",
            "ep: 158000, epsilon: 0.984\n",
            "Policy Q-learn: -94.04843567686365, mean\"322.93\n",
            "\n",
            "ep: 158500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"161.87\n",
            "\n",
            "ep: 159000, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"226.37\n",
            "\n",
            "ep: 159500, epsilon: 0.984\n",
            "Policy Q-learn: -92.00670574893786, mean\"349.28\n",
            "\n",
            "ep: 160000, epsilon: 0.984\n",
            "Policy Q-learn: -96.0997935100392, mean\"335.44\n",
            "\n",
            "ep: 160500, epsilon: 0.984\n",
            "Policy Q-learn: -98.9855660256861, mean\"377.28\n",
            "\n",
            "ep: 161000, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"164.07\n",
            "\n",
            "ep: 161500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"144.89\n",
            "\n",
            "ep: 162000, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"155.2\n",
            "\n",
            "ep: 162500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"136.97\n",
            "\n",
            "ep: 163000, epsilon: 0.984\n",
            "Policy Q-learn: -97.15650149782705, mean\"300.92\n",
            "\n",
            "ep: 163500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"217.01\n",
            "\n",
            "ep: 164000, epsilon: 0.984\n",
            "Policy Q-learn: -85.23818294255963, mean\"441.85\n",
            "\n",
            "ep: 164500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"168.64\n",
            "\n",
            "ep: 165000, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"147.42\n",
            "\n",
            "ep: 165500, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"216.67\n",
            "\n",
            "ep: 166000, epsilon: 0.984\n",
            "Policy Q-learn: -100.0, mean\"176.15\n",
            "\n",
            "ep: 166500, epsilon: 0.983\n",
            "Policy Q-learn: -93.0896611224273, mean\"299.61\n",
            "\n",
            "ep: 167000, epsilon: 0.983\n",
            "Policy Q-learn: -87.06897506928122, mean\"321.3\n",
            "\n",
            "ep: 167500, epsilon: 0.983\n",
            "Policy Q-learn: -99.00238056367867, mean\"243.8\n",
            "\n",
            "ep: 168000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"201.73\n",
            "\n",
            "ep: 168500, epsilon: 0.983\n",
            "Policy Q-learn: -91.03532616457161, mean\"363.69\n",
            "\n",
            "ep: 169000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"140.13\n",
            "\n",
            "ep: 169500, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"139.38\n",
            "\n",
            "ep: 170000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"194.83\n",
            "\n",
            "ep: 170500, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"274.63\n",
            "\n",
            "ep: 171000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"197.45\n",
            "\n",
            "ep: 171500, epsilon: 0.983\n",
            "Policy Q-learn: -99.0068452258072, mean\"205.15\n",
            "\n",
            "ep: 172000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"145.7\n",
            "\n",
            "ep: 172500, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"362.21\n",
            "\n",
            "ep: 173000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"182.64\n",
            "\n",
            "ep: 173500, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"121.62\n",
            "\n",
            "ep: 174000, epsilon: 0.983\n",
            "Policy Q-learn: -82.95069691688514, mean\"479.74\n",
            "\n",
            "ep: 174500, epsilon: 0.983\n",
            "Policy Q-learn: -88.99958986906731, mean\"434.05\n",
            "\n",
            "ep: 175000, epsilon: 0.983\n",
            "Policy Q-learn: -80.08459837698655, mean\"410.64\n",
            "\n",
            "ep: 175500, epsilon: 0.983\n",
            "Policy Q-learn: -86.10328864301957, mean\"364.0\n",
            "\n",
            "ep: 176000, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"119.54\n",
            "\n",
            "ep: 176500, epsilon: 0.983\n",
            "Policy Q-learn: -100.0, mean\"195.77\n",
            "\n",
            "ep: 177000, epsilon: 0.982\n",
            "Policy Q-learn: -95.01996570572726, mean\"317.61\n",
            "\n",
            "ep: 177500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"148.98\n",
            "\n",
            "ep: 178000, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"158.53\n",
            "\n",
            "ep: 178500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"196.21\n",
            "\n",
            "ep: 179000, epsilon: 0.982\n",
            "Policy Q-learn: -63.179771885292624, mean\"614.66\n",
            "\n",
            "ep: 179500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"170.09\n",
            "\n",
            "ep: 180000, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"199.22\n",
            "\n",
            "ep: 180500, epsilon: 0.982\n",
            "Policy Q-learn: -97.10699445776908, mean\"222.8\n",
            "\n",
            "ep: 181000, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"133.09\n",
            "\n",
            "ep: 181500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"242.37\n",
            "\n",
            "ep: 182000, epsilon: 0.982\n",
            "Policy Q-learn: -98.99240693520534, mean\"201.09\n",
            "\n",
            "ep: 182500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"111.11\n",
            "\n",
            "ep: 183000, epsilon: 0.982\n",
            "Policy Q-learn: -99.00313682821772, mean\"190.1\n",
            "\n",
            "ep: 183500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"153.62\n",
            "\n",
            "ep: 184000, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"149.5\n",
            "\n",
            "ep: 184500, epsilon: 0.982\n",
            "Policy Q-learn: -96.0057447381702, mean\"467.31\n",
            "\n",
            "ep: 185000, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"194.4\n",
            "\n",
            "ep: 185500, epsilon: 0.982\n",
            "Policy Q-learn: -98.98610400648606, mean\"220.34\n",
            "\n",
            "ep: 186000, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"128.98\n",
            "\n",
            "ep: 186500, epsilon: 0.982\n",
            "Policy Q-learn: -100.0, mean\"216.2\n",
            "\n",
            "ep: 187000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"163.01\n",
            "\n",
            "ep: 187500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"169.44\n",
            "\n",
            "ep: 188000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"143.95\n",
            "\n",
            "ep: 188500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"245.35\n",
            "\n",
            "ep: 189000, epsilon: 0.981\n",
            "Policy Q-learn: -99.02104593579213, mean\"241.89\n",
            "\n",
            "ep: 189500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"256.56\n",
            "\n",
            "ep: 190000, epsilon: 0.981\n",
            "Policy Q-learn: -92.06723298411649, mean\"347.66\n",
            "\n",
            "ep: 190500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"117.45\n",
            "\n",
            "ep: 191000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"148.09\n",
            "\n",
            "ep: 191500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"152.67\n",
            "\n",
            "ep: 192000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"262.77\n",
            "\n",
            "ep: 192500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"141.97\n",
            "\n",
            "ep: 193000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"182.22\n",
            "\n",
            "ep: 193500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"154.4\n",
            "\n",
            "ep: 194000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"196.58\n",
            "\n",
            "ep: 194500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"145.79\n",
            "\n",
            "ep: 195000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"142.02\n",
            "\n",
            "ep: 195500, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"292.16\n",
            "\n",
            "ep: 196000, epsilon: 0.981\n",
            "Policy Q-learn: -100.0, mean\"192.06\n",
            "\n",
            "ep: 196500, epsilon: 0.981\n",
            "Policy Q-learn: -98.77388935331379, mean\"186.99\n",
            "\n",
            "ep: 197000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"242.93\n",
            "\n",
            "ep: 197500, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"140.51\n",
            "\n",
            "ep: 198000, epsilon: 0.980\n",
            "Policy Q-learn: -93.98906942326278, mean\"335.21\n",
            "\n",
            "ep: 198500, epsilon: 0.980\n",
            "Policy Q-learn: -96.99909417314724, mean\"197.64\n",
            "\n",
            "ep: 199000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"184.42\n",
            "\n",
            "ep: 199500, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"196.0\n",
            "\n",
            "ep: 200000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"153.77\n",
            "\n",
            "ep: 200500, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"130.57\n",
            "\n",
            "ep: 201000, epsilon: 0.980\n",
            "Policy Q-learn: -98.99971752272565, mean\"241.59\n",
            "\n",
            "ep: 201500, epsilon: 0.980\n",
            "Policy Q-learn: -88.00203426828052, mean\"375.93\n",
            "\n",
            "ep: 202000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"328.59\n",
            "\n",
            "ep: 202500, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"238.21\n",
            "\n",
            "ep: 203000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"154.88\n",
            "\n",
            "ep: 203500, epsilon: 0.980\n",
            "Policy Q-learn: -66.2507064594522, mean\"556.2\n",
            "\n",
            "ep: 204000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"158.17\n",
            "\n",
            "ep: 204500, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"148.53\n",
            "\n",
            "ep: 205000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"220.14\n",
            "\n",
            "ep: 205500, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"145.96\n",
            "\n",
            "ep: 206000, epsilon: 0.980\n",
            "Policy Q-learn: -89.97465757833913, mean\"378.9\n",
            "\n",
            "ep: 206500, epsilon: 0.980\n",
            "Policy Q-learn: -97.99299142806329, mean\"266.52\n",
            "\n",
            "ep: 207000, epsilon: 0.980\n",
            "Policy Q-learn: -100.0, mean\"255.75\n",
            "\n",
            "ep: 207500, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"149.11\n",
            "\n",
            "ep: 208000, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"156.37\n",
            "\n",
            "ep: 208500, epsilon: 0.979\n",
            "Policy Q-learn: -99.00219185431386, mean\"193.26\n",
            "\n",
            "ep: 209000, epsilon: 0.979\n",
            "Policy Q-learn: -99.00095147182792, mean\"154.53\n",
            "\n",
            "ep: 209500, epsilon: 0.979\n",
            "Policy Q-learn: -85.05468379163536, mean\"395.34\n",
            "\n",
            "ep: 210000, epsilon: 0.979\n",
            "Policy Q-learn: -94.05865523126826, mean\"321.32\n",
            "\n",
            "ep: 210500, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"195.97\n",
            "\n",
            "ep: 211000, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"177.1\n",
            "\n",
            "ep: 211500, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"189.26\n",
            "\n",
            "ep: 212000, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"238.26\n",
            "\n",
            "ep: 212500, epsilon: 0.979\n",
            "Policy Q-learn: -79.01434505221468, mean\"480.34\n",
            "\n",
            "ep: 213000, epsilon: 0.979\n",
            "Policy Q-learn: -92.08355519557419, mean\"313.08\n",
            "\n",
            "ep: 213500, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"213.6\n",
            "\n",
            "ep: 214000, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"187.09\n",
            "\n",
            "ep: 214500, epsilon: 0.979\n",
            "Policy Q-learn: -97.01027330896959, mean\"314.34\n",
            "\n",
            "ep: 215000, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"299.43\n",
            "\n",
            "ep: 215500, epsilon: 0.979\n",
            "Policy Q-learn: -100.0, mean\"146.9\n",
            "\n",
            "ep: 216000, epsilon: 0.979\n",
            "Policy Q-learn: -94.03195753298718, mean\"323.9\n",
            "\n",
            "ep: 216500, epsilon: 0.979\n",
            "Policy Q-learn: -96.08349112923693, mean\"319.94\n",
            "\n",
            "ep: 217000, epsilon: 0.979\n",
            "Policy Q-learn: -97.00578785289515, mean\"292.0\n",
            "\n",
            "ep: 217500, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"233.65\n",
            "\n",
            "ep: 218000, epsilon: 0.978\n",
            "Policy Q-learn: -96.07404303482045, mean\"262.37\n",
            "\n",
            "ep: 218500, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"153.19\n",
            "\n",
            "ep: 219000, epsilon: 0.978\n",
            "Policy Q-learn: -98.99196521267015, mean\"168.31\n",
            "\n",
            "ep: 219500, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"148.45\n",
            "\n",
            "ep: 220000, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"162.51\n",
            "\n",
            "ep: 220500, epsilon: 0.978\n",
            "Policy Q-learn: -98.99994255314078, mean\"137.54\n",
            "\n",
            "ep: 221000, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"181.13\n",
            "\n",
            "ep: 221500, epsilon: 0.978\n",
            "Policy Q-learn: -91.10370189419325, mean\"488.43\n",
            "\n",
            "ep: 222000, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"198.1\n",
            "\n",
            "ep: 222500, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"274.14\n",
            "\n",
            "ep: 223000, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"146.71\n",
            "\n",
            "ep: 223500, epsilon: 0.978\n",
            "Policy Q-learn: -92.99775035387421, mean\"296.74\n",
            "\n",
            "ep: 224000, epsilon: 0.978\n",
            "Policy Q-learn: -64.08764851661941, mean\"550.59\n",
            "\n",
            "ep: 224500, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"239.83\n",
            "\n",
            "ep: 225000, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"160.04\n",
            "\n",
            "ep: 225500, epsilon: 0.978\n",
            "Policy Q-learn: -96.16773128919718, mean\"320.86\n",
            "\n",
            "ep: 226000, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"142.19\n",
            "\n",
            "ep: 226500, epsilon: 0.978\n",
            "Policy Q-learn: -96.11103556354472, mean\"310.44\n",
            "\n",
            "ep: 227000, epsilon: 0.978\n",
            "Policy Q-learn: -97.99356548512122, mean\"135.03\n",
            "\n",
            "ep: 227500, epsilon: 0.978\n",
            "Policy Q-learn: -100.0, mean\"167.63\n",
            "\n",
            "ep: 228000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"147.33\n",
            "\n",
            "ep: 228500, epsilon: 0.977\n",
            "Policy Q-learn: -95.09818203700479, mean\"335.74\n",
            "\n",
            "ep: 229000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"197.86\n",
            "\n",
            "ep: 229500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"147.39\n",
            "\n",
            "ep: 230000, epsilon: 0.977\n",
            "Policy Q-learn: -95.09826413077997, mean\"296.53\n",
            "\n",
            "ep: 230500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"166.01\n",
            "\n",
            "ep: 231000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"181.59\n",
            "\n",
            "ep: 231500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"250.78\n",
            "\n",
            "ep: 232000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"194.4\n",
            "\n",
            "ep: 232500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"201.3\n",
            "\n",
            "ep: 233000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"192.08\n",
            "\n",
            "ep: 233500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"155.14\n",
            "\n",
            "ep: 234000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"209.72\n",
            "\n",
            "ep: 234500, epsilon: 0.977\n",
            "Policy Q-learn: -93.01248507725263, mean\"301.04\n",
            "\n",
            "ep: 235000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"148.65\n",
            "\n",
            "ep: 235500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"198.37\n",
            "\n",
            "ep: 236000, epsilon: 0.977\n",
            "Policy Q-learn: -96.04134950193144, mean\"376.44\n",
            "\n",
            "ep: 236500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"126.42\n",
            "\n",
            "ep: 237000, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"155.4\n",
            "\n",
            "ep: 237500, epsilon: 0.977\n",
            "Policy Q-learn: -100.0, mean\"134.8\n",
            "\n",
            "ep: 238000, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"154.0\n",
            "\n",
            "ep: 238500, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"232.79\n",
            "\n",
            "ep: 239000, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"158.54\n",
            "\n",
            "ep: 239500, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"148.3\n",
            "\n",
            "ep: 240000, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"154.9\n",
            "\n",
            "ep: 240500, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"159.34\n",
            "\n",
            "ep: 241000, epsilon: 0.976\n",
            "Policy Q-learn: -100.0, mean\"218.33\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYlKbzvjlfcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#action space wrapper\n",
        "\n",
        "class ObservationWrapper(gym.ObservationWrapper):\n",
        "    '''\n",
        "    n_states(int): number of state of the env.\n",
        "                It has to be the some power of N = env.observation_space.shape[0]\n",
        "    bounds(list of tuples): bounds of the actions of the env.(taken from the gymnasium documentation).\n",
        "                each tuple gives the bounds of the states\n",
        "    '''\n",
        "    def __init__(self, env, n_states=4096, bounds=[(-4.8,4.8),(-50,50),( -0.418, 0.418),(-20,20)]):\n",
        "        super().__init__(env)\n",
        "\n",
        "        self.n_states = n_states\n",
        "        self.bounds = bounds\n",
        "        assert isinstance(env.observation_space, gym.spaces.box.Box)\n",
        "\n",
        "        # Get the dimensions of the observation space\n",
        "        self.dimension = env.observation_space.shape[0] \n",
        "        self.states_per_dimention = int(n_states**(1/self.dimension)) #we give equal number of state per dimension\n",
        "        \n",
        "        # calculating steps for each state and storing in a list\n",
        "        self.step_list = []\n",
        "        for state_bound in bounds:\n",
        "            step = (state_bound[1] - state_bound[0])/self.states_per_dimention\n",
        "            self.step_list.append(step)\n",
        "\n",
        "        \n",
        "    def get_quant_observation(self, new_obs):\n",
        "        ##calculate the new quantized state index from a list of states.  (like converting number systems)\n",
        "        # print(new_obs)\n",
        "        quantized_obs = 0\n",
        "        dimension = self.dimension\n",
        "        for i, obs in enumerate(new_obs):\n",
        "            quantized_obs+= obs*self.states_per_dimention**(dimension-i-1)\n",
        "        \n",
        "        # print(quantized_obs)\n",
        "        return int(quantized_obs)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        ### BEGIN SOLUTION\n",
        "        # Quantize the observations (states)\n",
        "        new_obs = []\n",
        "        # print(obs)\n",
        "        for i,observation_state in enumerate(obs):\n",
        "            new_obs.append(int((observation_state-self.bounds[i][0])/self.step_list[i]))\n",
        "            \n",
        "        # Once quantized each dimension compute a single observation index\n",
        "        return self.get_quant_observation(new_obs)\n"
      ],
      "metadata": {
        "id": "BgceZ-H8MZmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}